{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5e83af-e383-4984-b11d-92e801925857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, datediff, to_date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CourseAnalytics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e60909-e248-4eba-b209-c73241581f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|   4.0|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|  NULL|\n|    E003|  U001|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|  NULL|\n|    E004|  U003|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|   5.0|\n|    E005|  U004|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|   4.0|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n\n+--------+-------------+-------------+------------+\n|CourseID|   Instructor|DurationHours|       Level|\n+--------+-------------+-------------+------------+\n|    C001|Abdullah Khan|            8|    Beginner|\n|    C002|   Sana Gupta|            5|    Beginner|\n|    C003| Ibrahim Khan|           10|Intermediate|\n|    C004|  Zoya Sheikh|            6|    Beginner|\n+--------+-------------+-------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Load course_enrollments (with space and brackets in name)\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/Volumes/workspace/default/nithyashree/course_enrollments (1).csv\")\n",
    "\n",
    "# Load course_catalog\n",
    "df_catalog = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/Volumes/workspace/default/nithyashree/course_catalog.csv\")\n",
    "\n",
    "# Show data\n",
    "df.show()\n",
    "df_catalog.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "809fe214-8996-4d51-a4a3-3120ce0d6afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Ingestion & Time Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03fc0bb0-3b65-4078-b043-5bafe135f34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- EnrollID: string (nullable = true)\n |-- UserID: string (nullable = true)\n |-- CourseID: string (nullable = true)\n |-- CourseName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- EnrollDate: date (nullable = true)\n |-- CompletionDate: date (nullable = true)\n |-- ProgressPercent: integer (nullable = true)\n |-- Rating: double (nullable = true)\n\nroot\n |-- CourseID: string (nullable = true)\n |-- Instructor: string (nullable = true)\n |-- DurationHours: integer (nullable = true)\n |-- Level: string (nullable = true)\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|   4.0|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|  NULL|\n|    E003|  U001|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|  NULL|\n|    E004|  U003|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|   5.0|\n|    E005|  U004|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|   4.0|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Load the enrollments dataset (replace with correct path if needed)\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"/Volumes/workspace/default/nithyashree/course_enrollments (1).csv\"\n",
    ")\n",
    "\n",
    "# Load the course catalog dataset\n",
    "df_catalog = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"/Volumes/workspace/default/nithyashree/course_catalog.csv\"\n",
    ")\n",
    "\n",
    "# Show schemas\n",
    "df.printSchema()\n",
    "df_catalog.printSchema()\n",
    "\n",
    "# Convert date fields to DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df = df.withColumn(\"EnrollDate\", to_date(\"EnrollDate\", \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"CompletionDate\", to_date(\"CompletionDate\", \"yyyy-MM-dd\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f08b58d8-6387-4b7a-a334-64547f114082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add DaysToComplete Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a00a6b6b-a33e-447a-ac1d-4724f8b296ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------------+----------+--------------+--------------+\n|EnrollID|UserID|       CourseName|EnrollDate|CompletionDate|DaysToComplete|\n+--------+------+-----------------+----------+--------------+--------------+\n|    E001|  U001|    Python Basics|2024-04-01|    2024-04-10|             9|\n|    E002|  U002|Excel for Finance|2024-04-02|          NULL|          NULL|\n|    E003|  U001|  ML with PySpark|2024-04-03|          NULL|          NULL|\n|    E004|  U003|    Python Basics|2024-04-04|    2024-04-20|            16|\n|    E005|  U004|Digital Marketing|2024-04-05|    2024-04-16|            11|\n+--------+------+-----------------+----------+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "# Add column only for completed courses\n",
    "df = df.withColumn(\"DaysToComplete\", datediff(\"CompletionDate\", \"EnrollDate\"))\n",
    "\n",
    "df.select(\"EnrollID\", \"UserID\", \"CourseName\", \"EnrollDate\", \"CompletionDate\", \"DaysToComplete\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b71bfe08-3228-4349-9e76-ed0b416078ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " User Learning Path Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54df1454-25fe-404e-8960-0ab0b078215e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+\n|UserID|TotalCourses|AvgProgress|\n+------+------------+-----------+\n|  U002|           1|       45.0|\n|  U001|           2|       65.0|\n|  U004|           1|      100.0|\n|  U003|           1|      100.0|\n+------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count, when\n",
    "\n",
    "df = df.withColumn(\"IsCompleted\", (col(\"ProgressPercent\") == 100))\n",
    "\n",
    "user_progress = df.groupBy(\"UserID\") \\\n",
    "    .agg(count(\"*\").alias(\"TotalCourses\"),\n",
    "         avg(\"ProgressPercent\").alias(\"AvgProgress\"))\n",
    "\n",
    "user_progress.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d8a245-aa05-4348-a274-ed97e425f3ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Engagement Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87a75cf-cbb2-42ef-aa48-5dd2efb43152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------------+---------------+\n|EnrollID|UserID|       CourseName|EngagementScore|\n+--------+------+-----------------+---------------+\n|    E001|  U001|    Python Basics|          400.0|\n|    E002|  U002|Excel for Finance|            0.0|\n|    E003|  U001|  ML with PySpark|            0.0|\n|    E004|  U003|    Python Basics|          500.0|\n|    E005|  U004|Digital Marketing|          400.0|\n+--------+------+-----------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Rating\", when(col(\"Rating\").isNull(), 0).otherwise(col(\"Rating\")))\n",
    "df = df.withColumn(\"EngagementScore\", col(\"ProgressPercent\") * col(\"Rating\"))\n",
    "df.select(\"EnrollID\", \"UserID\", \"CourseName\", \"EngagementScore\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e76afbeb-8386-4cdb-bfb6-bc8e432bb64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Identify Drop-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1fe26fc-be9d-4f59-b00d-65f55df67c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|DaysToComplete|IsCompleted|EngagementScore|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|   0.0|          NULL|      false|            0.0|\n|    E003|  U001|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|   0.0|          NULL|      false|            0.0|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "dropouts = df.filter((col(\"ProgressPercent\") < 50) & col(\"CompletionDate\").isNull())\n",
    "dropouts.createOrReplaceTempView(\"Dropouts\")\n",
    "spark.sql(\"SELECT * FROM Dropouts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "972182a2-3817-4654-9e4f-e89b2eeeadb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Join with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5f6ae7-5a4c-4792-9c1e-0fb0b3c3b8bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n|   Instructor|AvgProgress|\n+-------------+-----------+\n| Ibrahim Khan|       30.0|\n|  Zoya Sheikh|      100.0|\n|Abdullah Khan|      100.0|\n|   Sana Gupta|       45.0|\n+-------------+-----------+\n\n+-------------+-------------+----------------+\n|   CourseName|   Instructor|TotalEnrollments|\n+-------------+-------------+----------------+\n|Python Basics|Abdullah Khan|               2|\n+-------------+-------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "joined = df.join(df_catalog, on=\"CourseID\", how=\"left\")\n",
    "joined.createOrReplaceTempView(\"JoinedTable\")\n",
    "\n",
    "# Avg progress per instructor\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Instructor, AVG(ProgressPercent) AS AvgProgress\n",
    "    FROM JoinedTable\n",
    "    GROUP BY Instructor\n",
    "\"\"\").show()\n",
    "\n",
    "# Most enrolled course instructor\n",
    "spark.sql(\"\"\"\n",
    "    SELECT CourseName, Instructor, COUNT(*) AS TotalEnrollments\n",
    "    FROM JoinedTable\n",
    "    GROUP BY CourseName, Instructor\n",
    "    ORDER BY TotalEnrollments DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b80d6a7-9ca9-4c73-bd33-e603be7d2608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Save as Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54605c6d-9224-4138-b57a-bf91b73e6206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"enrollments_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa11ed60-b04b-4593-a4a7-7f5c1a5402ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta Table Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7030241-f032-4738-a393-89bb9c61f6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update ratings for Python Basics\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO enrollments_delta AS target\n",
    "USING (\n",
    "  SELECT * FROM enrollments_delta WHERE CourseName = 'Python Basics'\n",
    ") AS source\n",
    "ON target.EnrollID = source.EnrollID\n",
    "WHEN MATCHED THEN UPDATE SET Rating = 5\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f2dc2b4-cd4a-4260-86b5-60ee3b27b0eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta Table Deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef1d141-fa8b-40dc-a9e9-de780f890776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete where ProgressPercent is 0\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM enrollments_delta WHERE ProgressPercent = 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c7ea8c8-cd32-4323-8ac6-ac6840549ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "View Delta History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102e6a9b-2fc9-4604-a543-8d36a4a8e912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------------+---------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+------------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+--------------------------------------------------+\n|version|timestamp          |userId          |userName                  |operation                        |operationParameters                                                                                                                                                                                        |job |notebook|clusterId               |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |userMetadata|engineInfo                                        |\n+-------+-------------------+----------------+--------------------------+---------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+------------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+--------------------------------------------------+\n|3      |2025-06-19 08:01:07|8832284645870584|nithyashreer2019@gmail.com|DELETE                           |{predicate -> [\"(ProgressPercent#13932 = 0)\"]}                                                                                                                                                             |NULL|NULL    |0619-074433-pxmuusme-v2n|2          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 222, numDeletionVectorsUpdated -> 0, numDeletedRows -> 0, scanTimeMs -> 217, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}                                                                                                                                                                                                                                                                                                                                                               |NULL        |Databricks-Runtime/16.4.x-aarch64-photon-scala2.12|\n|2      |2025-06-19 08:00:43|8832284645870584|nithyashreer2019@gmail.com|OPTIMIZE                         |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                             |NULL|NULL    |0619-074433-pxmuusme-v2n|1          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 6444, p25FileSize -> 3354, numDeletionVectorsRemoved -> 1, minFileSize -> 3354, numAddedFiles -> 1, maxFileSize -> 3354, p75FileSize -> 3354, p50FileSize -> 3354, numAddedBytes -> 3354}                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Databricks-Runtime/16.4.x-aarch64-photon-scala2.12|\n|1      |2025-06-19 08:00:40|8832284645870584|nithyashreer2019@gmail.com|MERGE                            |{predicate -> [\"(EnrollID#12541 = EnrollID#12553)\"], clusterBy -> [], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> []}|NULL|NULL    |0619-074433-pxmuusme-v2n|0          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 3089, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 2, executionTimeMs -> 3440, materializeSourceTimeMs -> 8, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1595, numTargetRowsUpdated -> 2, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1710}|NULL        |Databricks-Runtime/16.4.x-aarch64-photon-scala2.12|\n|0      |2025-06-19 08:00:14|8832284645870584|nithyashreer2019@gmail.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true}                                                    |NULL|NULL    |0619-074433-pxmuusme-v2n|NULL       |WriteSerializable|false        |{numFiles -> 1, numRemovedFiles -> 0, numRemovedBytes -> 0, numOutputRows -> 5, numOutputBytes -> 3355}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |NULL        |Databricks-Runtime/16.4.x-aarch64-photon-scala2.12|\n+-------+-------------------+----------------+--------------------------+---------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+------------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+--------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY enrollments_delta\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0b497db-233d-4e94-8b13-e329d0bf14a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Window Functions – Rank by Enrollments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d741d207-b97c-417a-a8ce-a596c018ee2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+----+\n|       CourseName|count|Rank|\n+-----------------+-----+----+\n|    Python Basics|    2|   1|\n|  ML with PySpark|    1|   2|\n|Excel for Finance|    1|   2|\n|Digital Marketing|    1|   2|\n+-----------------+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "course_counts = df.groupBy(\"CourseName\").count()\n",
    "window = Window.orderBy(col(\"count\").desc())\n",
    "\n",
    "ranked = course_counts.withColumn(\"Rank\", dense_rank().over(window))\n",
    "ranked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fde27f5-a5f7-44a0-8847-3cd29c9840d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Window Function – Lead for next course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8550172-8613-47c4-afbb-442799f0b846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+---------------+----------+\n|UserID|       CourseName|     NextCourse|EnrollDate|\n+------+-----------------+---------------+----------+\n|  U001|    Python Basics|ML with PySpark|2024-04-01|\n|  U001|  ML with PySpark|           NULL|2024-04-03|\n|  U002|Excel for Finance|           NULL|2024-04-02|\n|  U003|    Python Basics|           NULL|2024-04-04|\n|  U004|Digital Marketing|           NULL|2024-04-05|\n+------+-----------------+---------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "\n",
    "window = Window.partitionBy(\"UserID\").orderBy(\"EnrollDate\")\n",
    "next_course = df.withColumn(\"NextCourse\", lead(\"CourseName\").over(window))\n",
    "next_course.select(\"UserID\", \"CourseName\", \"NextCourse\", \"EnrollDate\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b77f50fa-545e-4c5e-aa4c-fa04c2b0e9ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " SQL Views for Dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f86e6d5-0f10-472d-92fa-85552782bee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Daily Enrollments\n+----------+----------------+\n|EnrollDate|TotalEnrollments|\n+----------+----------------+\n|2024-04-01|               1|\n|2024-04-02|               1|\n|2024-04-03|               1|\n|2024-04-04|               1|\n|2024-04-05|               1|\n+----------+----------------+\n\n Category Performance (Average Rating)\n+------------+---------+\n|    Category|AvgRating|\n+------------+---------+\n|Data Science|      0.0|\n| Programming|      4.5|\n|   Marketing|      4.0|\n|Productivity|      0.0|\n+------------+---------+\n\n Top 3 Courses\n+-----------------+-----+\n|       CourseName|Total|\n+-----------------+-----+\n|    Python Basics|    2|\n|Excel for Finance|    1|\n|  ML with PySpark|    1|\n+-----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create or replace a temp view\n",
    "df.createOrReplaceTempView(\"enrollments\")\n",
    "\n",
    "# 1. Daily Enrollments\n",
    "print(\" Daily Enrollments\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT EnrollDate, COUNT(*) AS TotalEnrollments\n",
    "    FROM enrollments\n",
    "    GROUP BY EnrollDate\n",
    "    ORDER BY EnrollDate\n",
    "\"\"\").show()\n",
    "\n",
    "# 2. Category Performance\n",
    "print(\" Category Performance (Average Rating)\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Category, AVG(Rating) AS AvgRating\n",
    "    FROM enrollments\n",
    "    GROUP BY Category\n",
    "\"\"\").show()\n",
    "\n",
    "# 3. Top 3 Courses by Enrollment Count\n",
    "print(\" Top 3 Courses\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT CourseName, COUNT(*) AS Total\n",
    "    FROM enrollments\n",
    "    GROUP BY CourseName\n",
    "    ORDER BY Total DESC\n",
    "    LIMIT 3\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c68e5ab5-ab9f-4c53-b2af-f7f89214eefe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Time Travel – View Previous Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1067053-49ff-400f-a707-1bf8764049dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|DaysToComplete|IsCompleted|EngagementScore|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|   4.0|             9|       true|          400.0|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|   0.0|          NULL|      false|            0.0|\n|    E003|  U001|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|   0.0|          NULL|      false|            0.0|\n|    E004|  U003|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|   5.0|            16|       true|          500.0|\n|    E005|  U004|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|   4.0|            11|       true|          400.0|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Replace <version_num> with actual value from DESCRIBE HISTORY\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"enrollments_delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5d881e2-31bf-4149-a645-69ecba7500d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Export Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102fdc7f-1d79-4985-9f02-19cac857cc8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "# Export as JSON partitioned by Category\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"Category\") \\\n",
    "    .json(\"/Volumes/workspace/default/nithyashree/exported_json\")\n",
    "\n",
    "# Summary dataframe\n",
    "summary = df.groupBy(\"CourseName\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"TotalEnrollments\"),\n",
    "        avg(\"Rating\").alias(\"AvgRating\"),\n",
    "        avg(\"ProgressPercent\").alias(\"AvgProgress\")\n",
    "    )\n",
    "\n",
    "# Export summary as Parquet\n",
    "summary.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"/Volumes/workspace/default/nithyashree/course_summary\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Course Analytics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "489f187c-6201-49c5-a692-a11c6f357cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Ingestion & Schema Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15f0256-08fc-48ea-a2be-dfb132eeb875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- LogID: string (nullable = true)\n |-- VehicleID: string (nullable = true)\n |-- EntryPoint: string (nullable = true)\n |-- ExitPoint: string (nullable = true)\n |-- EntryTime: timestamp (nullable = true)\n |-- ExitTime: timestamp (nullable = true)\n |-- VehicleType: string (nullable = true)\n |-- SpeedKMH: integer (nullable = true)\n |-- TollPaid: integer (nullable = true)\n\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n|LogID|VehicleID|EntryPoint|ExitPoint|EntryTime          |ExitTime           |VehicleType|SpeedKMH|TollPaid|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n|L001 |V001     |GateA     |GateC    |2024-05-01 08:01:00|2024-05-01 08:20:00|Car        |60      |50      |\n|L002 |V002     |GateB     |GateC    |2024-05-01 08:10:00|2024-05-01 08:45:00|Truck      |45      |100     |\n|L003 |V003     |GateA     |GateD    |2024-05-01 09:00:00|2024-05-01 09:18:00|Bike       |55      |30      |\n|L004 |V004     |GateC     |GateD    |2024-05-01 09:15:00|2024-05-01 09:35:00|Car        |80      |50      |\n|L005 |V005     |GateB     |GateA    |2024-05-01 10:05:00|2024-05-01 10:40:00|Bus        |40      |70      |\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True) \\\n",
    "    .csv(\"/Volumes/workspace/default/nithyashree/traffic_logs.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aed22b0-3a6a-4b35-afc9-e891cc24433e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- LogID: string (nullable = true)\n |-- VehicleID: string (nullable = true)\n |-- EntryPoint: string (nullable = true)\n |-- ExitPoint: string (nullable = true)\n |-- EntryTime: timestamp (nullable = true)\n |-- ExitTime: timestamp (nullable = true)\n |-- VehicleType: string (nullable = true)\n |-- SpeedKMH: integer (nullable = true)\n |-- TollPaid: integer (nullable = true)\n\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n|LogID|VehicleID|EntryPoint|ExitPoint|EntryTime          |ExitTime           |VehicleType|SpeedKMH|TollPaid|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n|L001 |V001     |GateA     |GateC    |2024-05-01 08:01:00|2024-05-01 08:20:00|Car        |60      |50      |\n|L002 |V002     |GateB     |GateC    |2024-05-01 08:10:00|2024-05-01 08:45:00|Truck      |45      |100     |\n|L003 |V003     |GateA     |GateD    |2024-05-01 09:00:00|2024-05-01 09:18:00|Bike       |55      |30      |\n|L004 |V004     |GateC     |GateD    |2024-05-01 09:15:00|2024-05-01 09:35:00|Car        |80      |50      |\n|L005 |V005     |GateB     |GateA    |2024-05-01 10:05:00|2024-05-01 10:40:00|Bus        |40      |70      |\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "traffic_schema = StructType([\n",
    "    StructField(\"LogID\", StringType(), True),\n",
    "    StructField(\"VehicleID\", StringType(), True),\n",
    "    StructField(\"EntryPoint\", StringType(), True),\n",
    "    StructField(\"ExitPoint\", StringType(), True),\n",
    "    StructField(\"EntryTime\", TimestampType(), True),\n",
    "    StructField(\"ExitTime\", TimestampType(), True),\n",
    "    StructField(\"VehicleType\", StringType(), True),\n",
    "    StructField(\"SpeedKMH\", IntegerType(), True),\n",
    "    StructField(\"TollPaid\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_manual = spark.read.option(\"header\", True).schema(traffic_schema) \\\n",
    "    .csv(\"/Volumes/workspace/default/nithyashree/traffic_logs.csv\")\n",
    "\n",
    "df_manual.printSchema()\n",
    "df_manual.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f43d2152-a8e0-48b1-9a91-d14ae35b472e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Derived Column Creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8665991-1f1b-4a50-8217-c39903462b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------------------+-----------+--------+\n|LogID|VehicleID|TripDurationMinutes|IsOverspeed|SpeedKMH|\n+-----+---------+-------------------+-----------+--------+\n| L001|     V001|                 19|      false|      60|\n| L002|     V002|                 35|      false|      45|\n| L003|     V003|                 18|      false|      55|\n| L004|     V004|                 20|       true|      80|\n| L005|     V005|                 35|      false|      40|\n+-----+---------+-------------------+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, when\n",
    "\n",
    "df2 = df_manual.withColumn(\n",
    "    \"TripDurationMinutes\",\n",
    "    (expr(\"unix_timestamp(ExitTime) - unix_timestamp(EntryTime)\") / 60).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"IsOverspeed\",\n",
    "    when(col(\"SpeedKMH\") > 60, True).otherwise(False)\n",
    ")\n",
    "\n",
    "df2.select(\"LogID\", \"VehicleID\", \"TripDurationMinutes\", \"IsOverspeed\", \"SpeedKMH\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a093199f-9a10-4275-ab89-d5f73f01876e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vehicle Behavior Aggregations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6584bade-fcb8-49a4-8f06-bb99b938ded0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n|VehicleType|AvgSpeed|\n+-----------+--------+\n|        Car|    70.0|\n|       Bike|    55.0|\n|      Truck|    45.0|\n|        Bus|    40.0|\n+-----------+--------+\n\n+----------+---------+\n|EntryPoint|TotalToll|\n+----------+---------+\n|     GateA|       80|\n|     GateC|       50|\n|     GateB|      170|\n+----------+---------+\n\n+---------+----------+\n|ExitPoint|UsageCount|\n+---------+----------+\n|    GateD|         2|\n+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, sum, count\n",
    "\n",
    "# 1. Average speed per VehicleType\n",
    "df2.groupBy(\"VehicleType\").agg(avg(\"SpeedKMH\").alias(\"AvgSpeed\")).show()\n",
    "\n",
    "# 2. Total toll collected per EntryPoint\n",
    "df2.groupBy(\"EntryPoint\").agg(sum(\"TollPaid\").alias(\"TotalToll\")).show()\n",
    "\n",
    "# 3. Most used ExitPoint\n",
    "df2.groupBy(\"ExitPoint\").agg(count(\"*\").alias(\"UsageCount\")) \\\n",
    "   .orderBy(col(\"UsageCount\").desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b61a2ed8-801b-4d12-a106-bbd4de3302f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d8c12f-ecdd-460c-b87c-e18bce60cd83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+---------+\n|VehicleID|VehicleType|SpeedKMH|SpeedRank|\n+---------+-----------+--------+---------+\n|     V003|       Bike|      55|        1|\n|     V005|        Bus|      40|        1|\n|     V004|        Car|      80|        1|\n|     V001|        Car|      60|        2|\n|     V002|      Truck|      45|        1|\n+---------+-----------+--------+---------+\n\n+---------+-------------------+-------------------+------------+\n|VehicleID|          EntryTime|           ExitTime|PrevExitTime|\n+---------+-------------------+-------------------+------------+\n|     V001|2024-05-01 08:01:00|2024-05-01 08:20:00|        NULL|\n|     V002|2024-05-01 08:10:00|2024-05-01 08:45:00|        NULL|\n|     V003|2024-05-01 09:00:00|2024-05-01 09:18:00|        NULL|\n|     V004|2024-05-01 09:15:00|2024-05-01 09:35:00|        NULL|\n|     V005|2024-05-01 10:05:00|2024-05-01 10:40:00|        NULL|\n+---------+-------------------+-------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, lag\n",
    "\n",
    "# 1. Rank vehicles by speed within each VehicleType\n",
    "w1 = Window.partitionBy(\"VehicleType\").orderBy(col(\"SpeedKMH\").desc())\n",
    "df2.withColumn(\"SpeedRank\", rank().over(w1)).select(\"VehicleID\", \"VehicleType\", \"SpeedKMH\", \"SpeedRank\").show()\n",
    "\n",
    "# 2. Find last exit time per VehicleID using lag\n",
    "w2 = Window.partitionBy(\"VehicleID\").orderBy(\"EntryTime\")\n",
    "df2.withColumn(\"PrevExitTime\", lag(\"ExitTime\").over(w2)).select(\"VehicleID\", \"EntryTime\", \"ExitTime\", \"PrevExitTime\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b2d880b-2067-4499-b094-6751575d2a7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Session Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad18b4ff-581a-4109-99f1-da5a5d565250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------+------------+\n|VehicleID|          EntryTime|PrevExitTime|IdleTimeMins|\n+---------+-------------------+------------+------------+\n|     V001|2024-05-01 08:01:00|        NULL|        NULL|\n|     V002|2024-05-01 08:10:00|        NULL|        NULL|\n|     V003|2024-05-01 09:00:00|        NULL|        NULL|\n|     V004|2024-05-01 09:15:00|        NULL|        NULL|\n|     V005|2024-05-01 10:05:00|        NULL|        NULL|\n+---------+-------------------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, round\n",
    "\n",
    "# Window by VehicleID ordered by EntryTime\n",
    "w = Window.partitionBy(\"VehicleID\").orderBy(\"EntryTime\")\n",
    "\n",
    "# Calculate idle time between trips\n",
    "session_df = df2.withColumn(\"PrevExitTime\", lag(\"ExitTime\").over(w)) \\\n",
    "    .withColumn(\"IdleTimeMins\", \n",
    "        round((unix_timestamp(\"EntryTime\") - unix_timestamp(\"PrevExitTime\")) / 60, 2))\n",
    "\n",
    "session_df.select(\"VehicleID\", \"EntryTime\", \"PrevExitTime\", \"IdleTimeMins\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24326d03-4169-40fd-8447-70f4e5d2b270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Anomaly Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "293aff60-4da0-40ab-b18a-6b130689095d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n|LogID|VehicleID|EntryPoint|ExitPoint|EntryTime|ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n|LogID|VehicleID|EntryPoint|ExitPoint|EntryTime|ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+\n\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|                 35|      false|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Vehicles with speed > 70 and trip duration < 10 minutes\n",
    "anomaly_fast_short = df2.filter((col(\"SpeedKMH\") > 70) & (col(\"TripDurationMinutes\") < 10))\n",
    "anomaly_fast_short.show()\n",
    "\n",
    "# 2. Vehicles that paid less toll for longer trips (e.g., > 30 mins and toll < 50)\n",
    "anomaly_low_toll = df2.filter((col(\"TripDurationMinutes\") > 30) & (col(\"TollPaid\") < 50))\n",
    "anomaly_low_toll.show()\n",
    "\n",
    "# 3. Suspicious backtracking (ExitPoint alphabetically before EntryPoint)\n",
    "anomaly_backtrack = df2.filter(col(\"ExitPoint\") < col(\"EntryPoint\"))\n",
    "anomaly_backtrack.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb4513ff-9961-4fd6-87b3-8b22cabba53d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Join with Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f30cdc0b-0f0d-49b6-9b5b-fe7e38111730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+-----------+--------------+\n|VehicleID|LogID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|OwnerName|      Model|RegisteredCity|\n+---------+-----+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+-----------+--------------+\n|     V001| L001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|                 19|      false|     Anil|Hyundai i20|         Delhi|\n|     V002| L002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|                 35|      false|   Rakesh| Tata Truck|       Chennai|\n|     V003| L003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|                 18|      false|     Sana| Yamaha R15|        Mumbai|\n|     V004| L004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|                 20|       true|     Neha| Honda City|     Bangalore|\n|     V005| L005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|                 35|      false|     Zoya|  Volvo Bus|          Pune|\n+---------+-----+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+-----------+--------------+\n\n+--------------+----------+\n|RegisteredCity|TotalTrips|\n+--------------+----------+\n|     Bangalore|         1|\n|       Chennai|         1|\n|         Delhi|         1|\n|        Mumbai|         1|\n|          Pune|         1|\n+--------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Load vehicle_registry.csv\n",
    "vehicle_df = spark.read.option(\"header\", True).csv(\"/Volumes/workspace/default/nithyashree/vehicle_registry.csv\")\n",
    "\n",
    "# Join on VehicleID\n",
    "enriched_df = df2.join(vehicle_df, on=\"VehicleID\", how=\"left\")\n",
    "enriched_df.show()\n",
    "\n",
    "# Group by RegisteredCity\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "city_summary = enriched_df.groupBy(\"RegisteredCity\").agg(count(\"*\").alias(\"TotalTrips\"))\n",
    "city_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16e7dd48-4d8d-4988-b4e4-b30f9efa66e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Delta Lake Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9453cff0-8d4f-4b10-bd91-61291e374e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------+--------------------+---------+--------------------+----+--------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|          userId|            userName|operation| operationParameters| job|notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+----------------+--------------------+---------+--------------------+----+--------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      3|2025-06-19 09:40:...|8832284645870584|nithyashreer2019@...| OPTIMIZE|{predicate -> [],...|NULL|    NULL|0619-090812-ipxtt...|          1|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      2| 2025-06-19 09:40:01|8832284645870584|nithyashreer2019@...|   DELETE|{predicate -> [\"(...|NULL|    NULL|0619-090812-ipxtt...|          1|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      1| 2025-06-19 09:39:59|8832284645870584|nithyashreer2019@...|    MERGE|{predicate -> [\"(...|NULL|    NULL|0619-090812-ipxtt...|          0|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      0| 2025-06-19 09:39:56|8832284645870584|nithyashreer2019@...|    WRITE|{mode -> Overwrit...|NULL|    NULL|0619-090812-ipxtt...|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+--------------------+----------------+--------------------+---------+--------------------+----+--------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Save DataFrame as Delta table\n",
    "enriched_df.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/workspace/default/nithyashree/traffic_delta\")\n",
    "\n",
    "# Load DeltaTable\n",
    "delta_table = DeltaTable.forPath(spark, \"/Volumes/workspace/default/nithyashree/traffic_delta\")\n",
    "\n",
    "# Step 1: Update toll for Bikes to 35 using MERGE INTO\n",
    "bike_updates = enriched_df.filter(col(\"VehicleType\") == \"Bike\").withColumn(\"TollPaid\", lit(35))\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source=bike_updates.alias(\"source\"),\n",
    "    condition=\"target.VehicleID = source.VehicleID AND target.EntryTime = source.EntryTime\"\n",
    ").whenMatchedUpdate(set={\"TollPaid\": col(\"source.TollPaid\")}).execute()\n",
    "\n",
    "# Step 2: Delete trips longer than 60 mins\n",
    "delta_table.delete(\"TripDurationMinutes > 60\")\n",
    "\n",
    "# Step 3: Describe Delta Table History\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/Volumes/workspace/default/nithyashree/traffic_delta`\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9ddfbda-656d-4a0f-988b-d3f4875af774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Advanced Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d946d7ec-7dda-4ef5-b7a1-5288ad64825f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, to_date, count\n",
    "\n",
    "# 1. Tag trip type as \"Short\", \"Medium\", or \"Long\" based on TripDurationMinutes\n",
    "enriched_df = enriched_df.withColumn(\n",
    "    \"TripType\",\n",
    "    when(col(\"TripDurationMinutes\") < 15, \"Short\")\n",
    "    .when((col(\"TripDurationMinutes\") >= 15) & (col(\"TripDurationMinutes\") <= 30), \"Medium\")\n",
    "    .otherwise(\"Long\")\n",
    ")\n",
    "\n",
    "# 2. Extract EntryDate for daily trip aggregation\n",
    "enriched_df = enriched_df.withColumn(\"EntryDate\", to_date(\"EntryTime\"))\n",
    "\n",
    "# 3. Flag vehicles with more than 3 trips in a day\n",
    "trip_counts = enriched_df.groupBy(\"VehicleID\", \"EntryDate\") \\\n",
    "    .agg(count(\"*\").alias(\"DailyTripCount\"))\n",
    "\n",
    "# Join back to flag frequent vehicles\n",
    "enriched_df = enriched_df.join(trip_counts, on=[\"VehicleID\", \"EntryDate\"], how=\"left\") \\\n",
    "    .withColumn(\"FrequentVehicle\", when(col(\"DailyTripCount\") > 3, True).otherwise(False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f99d0cd-38fe-4e61-91f2-2ffffe40f26e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Export & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f6320d-a379-45af-bc38-7774865a7f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+\n|VehicleType|ExitPoint|TotalToll|\n+-----------+---------+---------+\n|        Car|    GateC|       50|\n|       Bike|    GateD|       30|\n|      Truck|    GateC|      100|\n|        Car|    GateD|       50|\n|        Bus|    GateA|       70|\n+-----------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# Export 1: Write to Parquet partitioned by VehicleType\n",
    "enriched_df.write.mode(\"overwrite\").partitionBy(\"VehicleType\") \\\n",
    "    .parquet(\"/Volumes/workspace/default/nithyashree/traffic_output/parquet\")\n",
    "\n",
    "# Export 2: Write to CSV (for dashboards)\n",
    "enriched_df.write.mode(\"overwrite\").option(\"header\", True) \\\n",
    "    .csv(\"/Volumes/workspace/default/nithyashree/traffic_output/csv\")\n",
    "\n",
    "# Export 3: Register SQL view for dashboard summaries\n",
    "enriched_df.createOrReplaceTempView(\"traffic_summary\")\n",
    "\n",
    "# Sample View: Total toll by VehicleType and ExitPoint\n",
    "spark.sql(\"\"\"\n",
    "    SELECT VehicleType, ExitPoint, SUM(TollPaid) AS TotalToll\n",
    "    FROM traffic_summary\n",
    "    GROUP BY VehicleType, ExitPoint\n",
    "\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Set1_Solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
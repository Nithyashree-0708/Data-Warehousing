{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w_hwRLCTCaLS"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark -q\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySparkMasterTask\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "hkshNYZKE0Zl",
        "outputId": "dd9d555a-9781-4d1a-a5e2-a739444500eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-31fda28b-59a2-4383-baf5-a9fc969901cc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-31fda28b-59a2-4383-baf5-a9fc969901cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving orders.csv to orders (1).csv\n",
            "Saving customers.csv to customers.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load customers.csv\n",
        "customers_df = spark.read.option(\"header\", \"true\") \\\n",
        "                         .option(\"inferSchema\", \"true\") \\\n",
        "                         .csv(\"customers.csv\")\n",
        "\n",
        "# Load orders.csv\n",
        "orders_df = spark.read.option(\"header\", \"true\") \\\n",
        "                      .option(\"inferSchema\", \"true\") \\\n",
        "                      .csv(\"orders.csv\")\n",
        "\n",
        "# Show a preview\n",
        "customers_df.show()\n",
        "orders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PPl13OfE8zv",
        "outputId": "1bd6db16-831a-4c08-886f-c58e0144ac6d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----------------+---------+----------+\n",
            "|CustomerID| Name|            Email|     City|SignupDate|\n",
            "+----------+-----+-----------------+---------+----------+\n",
            "|       101|  Ali|    ali@gmail.com|   Mumbai|2022-05-10|\n",
            "|       102| Neha|   neha@yahoo.com|    Delhi|2023-01-15|\n",
            "|       103| Ravi| ravi@hotmail.com|Bangalore|2021-11-01|\n",
            "|       104|Sneha|sneha@outlook.com|Hyderabad|2020-07-22|\n",
            "|       105| Amit|   amit@gmail.com|  Chennai|2023-03-10|\n",
            "+----------+-----+-----------------+---------+----------+\n",
            "\n",
            "+-------+----------+---------+-----------+--------+-------+----------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 1: Data Ingestion &** **Exploration**"
      ],
      "metadata": {
        "id": "a04o8UXcFkDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns and DataTypes\n",
        "customers_df.printSchema()\n",
        "orders_df.printSchema()\n",
        "\n",
        "# Total Count\n",
        "customers_df.count()\n",
        "orders_df.count()\n",
        "\n",
        "# Distinct Cities\n",
        "customers_df.select(\"City\").distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYzzVd1_FoLA",
        "outputId": "fd6e73f7-894a-4bea-e75a-996c0b1d9b7a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Email: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- SignupDate: date (nullable = true)\n",
            "\n",
            "root\n",
            " |-- OrderID: integer (nullable = true)\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- OrderDate: date (nullable = true)\n",
            "\n",
            "+---------+\n",
            "|     City|\n",
            "+---------+\n",
            "|Bangalore|\n",
            "|  Chennai|\n",
            "|   Mumbai|\n",
            "|    Delhi|\n",
            "|Hyderabad|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: DataFrame Transformations**"
      ],
      "metadata": {
        "id": "JrnOggL9HphT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, year\n",
        "\n",
        "# Add TotalAmount = Price * Quantity\n",
        "orders_df = orders_df.withColumn(\"TotalAmount\", col(\"Price\") * col(\"Quantity\"))\n",
        "\n",
        "# Create OrderYear from OrderDate\n",
        "orders_df = orders_df.withColumn(\"OrderYear\", year(col(\"OrderDate\")))\n",
        "\n",
        "# Filter orders with TotalAmount > 10,000\n",
        "high_value_orders_df = orders_df.filter(col(\"TotalAmount\") > 10000)\n",
        "\n",
        "# Drop Email column from customers\n",
        "customers_no_email_df = customers_df.drop(\"Email\")\n",
        "\n",
        "# Show results\n",
        "print(\"Orders with TotalAmount > 10,000:\")\n",
        "high_value_orders_df.show()\n",
        "\n",
        "print(\"Customers DataFrame without Email column:\")\n",
        "customers_no_email_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUteYsJzH9K2",
        "outputId": "f000b19d-463c-4110-a2d2-328a4c7b8db5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orders with TotalAmount > 10,000:\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "|OrderID|CustomerID|Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "|      1|       101| Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|\n",
            "|      3|       102| Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|\n",
            "|      7|       102|  Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "\n",
            "Customers DataFrame without Email column:\n",
            "+----------+-----+---------+----------+\n",
            "|CustomerID| Name|     City|SignupDate|\n",
            "+----------+-----+---------+----------+\n",
            "|       101|  Ali|   Mumbai|2022-05-10|\n",
            "|       102| Neha|    Delhi|2023-01-15|\n",
            "|       103| Ravi|Bangalore|2021-11-01|\n",
            "|       104|Sneha|Hyderabad|2020-07-22|\n",
            "|       105| Amit|  Chennai|2023-03-10|\n",
            "+----------+-----+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Handling Nulls &** **Conditionals**"
      ],
      "metadata": {
        "id": "KTPodPa-IN3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, lit, to_date\n",
        "\n",
        "# Simulate a null in City for one customer (e.g., CustomerID=103)\n",
        "customers_null_city_df = customers_df.withColumn(\n",
        "    \"City\",\n",
        "    when(col(\"CustomerID\") == 103, None).otherwise(col(\"City\"))\n",
        ")\n",
        "\n",
        "# Fill nulls in City with \"Unknown\"\n",
        "customers_filled_city_df = customers_null_city_df.fillna({\"City\": \"Unknown\"})\n",
        "\n",
        "# Label customers as \"Loyal\" if SignupDate before 2022, else \"New\"\n",
        "customers_labeled_df = customers_filled_city_df.withColumn(\n",
        "    \"CustomerLabel\",\n",
        "    when(to_date(col(\"SignupDate\")) < lit(\"2022-01-01\"), \"Loyal\").otherwise(\"New\")\n",
        ")\n",
        "\n",
        "# Create OrderType: \"Low\" if TotalAmount < 5000, \"High\" if >= 5000\n",
        "orders_labeled_df = orders_df.withColumn(\n",
        "    \"OrderType\",\n",
        "    when(col(\"TotalAmount\") < 5000, \"Low\").otherwise(\"High\")\n",
        ")\n",
        "\n",
        "# Show results\n",
        "print(\"Customers with City null simulated and filled, and labeled:\")\n",
        "customers_labeled_df.show()\n",
        "\n",
        "print(\"Orders with OrderType column:\")\n",
        "orders_labeled_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1PRJswuIU8x",
        "outputId": "539c7631-59dc-4b58-e234-38c9969e40d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers with City null simulated and filled, and labeled:\n",
            "+----------+-----+-----------------+---------+----------+-------------+\n",
            "|CustomerID| Name|            Email|     City|SignupDate|CustomerLabel|\n",
            "+----------+-----+-----------------+---------+----------+-------------+\n",
            "|       101|  Ali|    ali@gmail.com|   Mumbai|2022-05-10|          New|\n",
            "|       102| Neha|   neha@yahoo.com|    Delhi|2023-01-15|          New|\n",
            "|       103| Ravi| ravi@hotmail.com|  Unknown|2021-11-01|        Loyal|\n",
            "|       104|Sneha|sneha@outlook.com|Hyderabad|2020-07-22|        Loyal|\n",
            "|       105| Amit|   amit@gmail.com|  Chennai|2023-03-10|          New|\n",
            "+----------+-----+-----------------+---------+----------+-------------+\n",
            "\n",
            "Orders with OrderType column:\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|OrderType|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|     High|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|      Low|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|     High|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|      Low|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|     High|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|     2024|      Low|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|     High|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4: Joins & Aggregations**"
      ],
      "metadata": {
        "id": "KFC6SK1NIa_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum as spark_sum, desc, count\n",
        "\n",
        "# Join customers and orders on CustomerID\n",
        "joined_df = customers_df.join(orders_df, on=\"CustomerID\", how=\"inner\")\n",
        "\n",
        "# Total orders and revenue per city\n",
        "orders_per_city = joined_df.groupBy(\"City\") \\\n",
        "                          .agg(\n",
        "                              count(\"OrderID\").alias(\"TotalOrders\"),\n",
        "                              spark_sum(\"TotalAmount\").alias(\"TotalRevenue\")\n",
        "                          ) \\\n",
        "                          .orderBy(desc(\"TotalRevenue\"))\n",
        "\n",
        "# Top 3 customers by total spend\n",
        "top_customers = joined_df.groupBy(\"CustomerID\", \"Name\") \\\n",
        "                         .agg(spark_sum(\"TotalAmount\").alias(\"TotalSpend\")) \\\n",
        "                         .orderBy(desc(\"TotalSpend\")) \\\n",
        "                         .limit(3)\n",
        "\n",
        "# Count how many products each category has sold\n",
        "products_sold = orders_df.groupBy(\"Category\") \\\n",
        "                        .agg(spark_sum(\"Quantity\").alias(\"TotalQuantity\")) \\\n",
        "                        .orderBy(desc(\"TotalQuantity\"))\n",
        "\n",
        "# Show results\n",
        "print(\"Total Orders and Revenue per City:\")\n",
        "orders_per_city.show()\n",
        "\n",
        "print(\"Top 3 Customers by Total Spend:\")\n",
        "top_customers.show()\n",
        "\n",
        "print(\"Total Products Sold per Category:\")\n",
        "products_sold.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vmbngpiImx2",
        "outputId": "b828b07e-724f-42d3-c9bf-a59314c0bbb7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Orders and Revenue per City:\n",
            "+---------+-----------+------------+\n",
            "|     City|TotalOrders|TotalRevenue|\n",
            "+---------+-----------+------------+\n",
            "|   Mumbai|          2|    101200.0|\n",
            "|    Delhi|          2|     50000.0|\n",
            "|Hyderabad|          1|      5000.0|\n",
            "|Bangalore|          1|      3500.0|\n",
            "|  Chennai|          1|      2500.0|\n",
            "+---------+-----------+------------+\n",
            "\n",
            "Top 3 Customers by Total Spend:\n",
            "+----------+-----+----------+\n",
            "|CustomerID| Name|TotalSpend|\n",
            "+----------+-----+----------+\n",
            "|       101|  Ali|  101200.0|\n",
            "|       102| Neha|   50000.0|\n",
            "|       104|Sneha|    5000.0|\n",
            "+----------+-----+----------+\n",
            "\n",
            "Total Products Sold per Category:\n",
            "+-----------+-------------+\n",
            "|   Category|TotalQuantity|\n",
            "+-----------+-------------+\n",
            "| Stationery|            5|\n",
            "|Electronics|            5|\n",
            "|  Furniture|            1|\n",
            "| Appliances|            1|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5: Spark SQL Tasks**"
      ],
      "metadata": {
        "id": "xp75Jg3fItZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create database sales and switch to it\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS sales\")\n",
        "spark.sql(\"USE sales\")\n",
        "\n",
        "# Save customers and orders as tables\n",
        "customers_df.write.mode(\"overwrite\").saveAsTable(\"customers\")\n",
        "orders_df.write.mode(\"overwrite\").saveAsTable(\"orders\")\n",
        "\n",
        "# 1. List all orders by customers from \"Delhi\"\n",
        "delhi_orders = spark.sql(\"\"\"\n",
        "    SELECT o.*\n",
        "    FROM orders o\n",
        "    JOIN customers c ON o.CustomerID = c.CustomerID\n",
        "    WHERE c.City = 'Delhi'\n",
        "\"\"\")\n",
        "print(\"Orders by customers from Delhi:\")\n",
        "delhi_orders.show()\n",
        "\n",
        "# 2. Find average order value in each category\n",
        "avg_order_value = spark.sql(\"\"\"\n",
        "    SELECT Category, AVG(Price * Quantity) as AvgOrderValue\n",
        "    FROM orders\n",
        "    GROUP BY Category\n",
        "\"\"\")\n",
        "print(\"Average order value per category:\")\n",
        "avg_order_value.show()\n",
        "\n",
        "# 3. Create view monthly_orders with month-wise total amount\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE VIEW monthly_orders AS\n",
        "    SELECT\n",
        "        MONTH(OrderDate) as OrderMonth,\n",
        "        SUM(Price * Quantity) as TotalAmount\n",
        "    FROM orders\n",
        "    GROUP BY MONTH(OrderDate)\n",
        "\"\"\")\n",
        "\n",
        "# Query the view\n",
        "print(\"Monthly Orders Summary:\")\n",
        "spark.sql(\"SELECT * FROM monthly_orders ORDER BY OrderMonth\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jvsf6zJI1HB",
        "outputId": "51e174c1-b052-4b35-b9b0-039ce0e0443c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orders by customers from Delhi:\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "|OrderID|CustomerID|Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "|      3|       102| Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|\n",
            "|      7|       102|  Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "\n",
            "Average order value per category:\n",
            "+-----------+-------------+\n",
            "|   Category|AvgOrderValue|\n",
            "+-----------+-------------+\n",
            "| Stationery|       2500.0|\n",
            "|Electronics|      37800.0|\n",
            "|  Furniture|       3500.0|\n",
            "| Appliances|       5000.0|\n",
            "+-----------+-------------+\n",
            "\n",
            "Monthly Orders Summary:\n",
            "+----------+-----------+\n",
            "|OrderMonth|TotalAmount|\n",
            "+----------+-----------+\n",
            "|         1|   101200.0|\n",
            "|         2|    28500.0|\n",
            "|         3|    32500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6: String & Date Functions**"
      ],
      "metadata": {
        "id": "zcuXBVKuJLxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace, concat_ws, datediff, current_date, month, date_format\n",
        "\n",
        "# Mask emails (e.g., a***@gmail.com)\n",
        "def mask_email(email):\n",
        "    import re\n",
        "    if email:\n",
        "        return re.sub(r'^(.)(.*)(.@.*)$', lambda m: m.group(1) + '***' + m.group(3), email)\n",
        "    return email\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "mask_email_udf = udf(mask_email, StringType())\n",
        "customers_masked_email_df = customers_df.withColumn(\"MaskedEmail\", mask_email_udf(\"Email\"))\n",
        "\n",
        "# Concatenate Name and City as \"Name from City\"\n",
        "customers_concat_df = customers_masked_email_df.withColumn(\n",
        "    \"NameFromCity\",\n",
        "    concat_ws(\" from \", col(\"Name\"), col(\"City\"))\n",
        ")\n",
        "\n",
        "# Calculate customer age in days (difference between current_date and SignupDate)\n",
        "customers_age_df = customers_concat_df.withColumn(\n",
        "    \"CustomerAgeDays\",\n",
        "    datediff(current_date(), col(\"SignupDate\"))\n",
        ")\n",
        "\n",
        "# Extract month name from OrderDate\n",
        "orders_monthname_df = orders_df.withColumn(\n",
        "    \"OrderMonthName\",\n",
        "    date_format(col(\"OrderDate\"), \"MMMM\")\n",
        ")\n",
        "\n",
        "# Show results\n",
        "print(\"Customers with masked email, concatenated name and city, and customer age:\")\n",
        "customers_age_df.select(\"Name\", \"Email\", \"MaskedEmail\", \"NameFromCity\", \"SignupDate\", \"CustomerAgeDays\").show()\n",
        "\n",
        "print(\"Orders with extracted month name:\")\n",
        "orders_monthname_df.select(\"OrderID\", \"OrderDate\", \"OrderMonthName\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMUkn22zJW9S",
        "outputId": "ecc8f1b6-31a5-4317-939e-b7431085d678"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers with masked email, concatenated name and city, and customer age:\n",
            "+-----+-----------------+-----------------+--------------------+----------+---------------+\n",
            "| Name|            Email|      MaskedEmail|        NameFromCity|SignupDate|CustomerAgeDays|\n",
            "+-----+-----------------+-----------------+--------------------+----------+---------------+\n",
            "|  Ali|    ali@gmail.com|  a***i@gmail.com|     Ali from Mumbai|2022-05-10|           1126|\n",
            "| Neha|   neha@yahoo.com|  n***a@yahoo.com|     Neha from Delhi|2023-01-15|            876|\n",
            "| Ravi| ravi@hotmail.com|r***i@hotmail.com| Ravi from Bangalore|2021-11-01|           1316|\n",
            "|Sneha|sneha@outlook.com|s***a@outlook.com|Sneha from Hyderabad|2020-07-22|           1783|\n",
            "| Amit|   amit@gmail.com|  a***t@gmail.com|   Amit from Chennai|2023-03-10|            822|\n",
            "+-----+-----------------+-----------------+--------------------+----------+---------------+\n",
            "\n",
            "Orders with extracted month name:\n",
            "+-------+----------+--------------+\n",
            "|OrderID| OrderDate|OrderMonthName|\n",
            "+-------+----------+--------------+\n",
            "|      1|2024-01-10|       January|\n",
            "|      2|2024-01-15|       January|\n",
            "|      3|2024-02-01|      February|\n",
            "|      4|2024-02-10|      February|\n",
            "|      5|2024-02-15|      February|\n",
            "|      6|2024-03-01|         March|\n",
            "|      7|2024-03-02|         March|\n",
            "+-------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 7: UDFs and Complex Logic**"
      ],
      "metadata": {
        "id": "iEvpyeXTJk-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "# UDF to tag customers based on total spend\n",
        "def tag_customer(spend):\n",
        "    if spend > 50000:\n",
        "        return \"Gold\"\n",
        "    elif spend >= 10000:\n",
        "        return \"Silver\"\n",
        "    else:\n",
        "        return \"Bronze\"\n",
        "\n",
        "tag_customer_udf = udf(tag_customer, StringType())\n",
        "\n",
        "# Calculate total spend per customer first\n",
        "total_spend_df = orders_df.groupBy(\"CustomerID\").agg({\"TotalAmount\": \"sum\"}) \\\n",
        "    .withColumnRenamed(\"sum(TotalAmount)\", \"TotalSpend\")\n",
        "\n",
        "# Join total spend with customers\n",
        "customers_spend_df = customers_df.join(total_spend_df, on=\"CustomerID\", how=\"left\") \\\n",
        "    .fillna({\"TotalSpend\": 0})\n",
        "\n",
        "# Apply tagging UDF\n",
        "customers_tagged_df = customers_spend_df.withColumn(\"CustomerTag\", tag_customer_udf(\"TotalSpend\"))\n",
        "\n",
        "# UDF to shorten product names (first 3 letters + \"...\")\n",
        "def shorten_product(name):\n",
        "    if name and len(name) > 3:\n",
        "        return name[:3] + \"...\"\n",
        "    return name\n",
        "\n",
        "shorten_product_udf = udf(shorten_product, StringType())\n",
        "\n",
        "# Apply product name shortening to orders\n",
        "orders_shortened_df = orders_df.withColumn(\"ShortProduct\", shorten_product_udf(\"Product\"))\n",
        "\n",
        "# Show results\n",
        "print(\"Customers with Tags:\")\n",
        "customers_tagged_df.select(\"CustomerID\", \"Name\", \"TotalSpend\", \"CustomerTag\").show()\n",
        "\n",
        "print(\"Orders with Shortened Product Names:\")\n",
        "orders_shortened_df.select(\"OrderID\", \"Product\", \"ShortProduct\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuVVLJklJ3Ao",
        "outputId": "01ccefed-cb0c-463e-ddaa-dc96b43fac7f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers with Tags:\n",
            "+----------+-----+----------+-----------+\n",
            "|CustomerID| Name|TotalSpend|CustomerTag|\n",
            "+----------+-----+----------+-----------+\n",
            "|       101|  Ali|  101200.0|       Gold|\n",
            "|       102| Neha|   50000.0|     Silver|\n",
            "|       103| Ravi|    3500.0|     Bronze|\n",
            "|       104|Sneha|    5000.0|     Bronze|\n",
            "|       105| Amit|    2500.0|     Bronze|\n",
            "+----------+-----+----------+-----------+\n",
            "\n",
            "Orders with Shortened Product Names:\n",
            "+-------+---------+------------+\n",
            "|OrderID|  Product|ShortProduct|\n",
            "+-------+---------+------------+\n",
            "|      1|   Laptop|      Lap...|\n",
            "|      2|    Mouse|      Mou...|\n",
            "|      3|   Tablet|      Tab...|\n",
            "|      4|Bookshelf|      Boo...|\n",
            "|      5|    Mixer|      Mix...|\n",
            "|      6| Notebook|      Not...|\n",
            "|      7|    Phone|      Pho...|\n",
            "+-------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 8: Parquet & Views**"
      ],
      "metadata": {
        "id": "kogKkspcKOe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the joined DataFrame (customers + orders) as Parquet\n",
        "joined_df.write.mode(\"overwrite\").parquet(\"joined_customers_orders.parquet\")\n",
        "\n",
        "# Read it back and verify schema\n",
        "parquet_df = spark.read.parquet(\"joined_customers_orders.parquet\")\n",
        "print(\"Schema of Parquet file:\")\n",
        "parquet_df.printSchema()\n",
        "\n",
        "# Create a global temp view\n",
        "parquet_df.createGlobalTempView(\"global_joined_view\")\n",
        "\n",
        "# Query the global temp view\n",
        "print(\"Data from global temp view:\")\n",
        "spark.sql(\"SELECT * FROM global_temp.global_joined_view\").show(5)\n",
        "\n",
        "# Performance comparison: CSV vs Parquet\n",
        "import time\n",
        "\n",
        "# CSV read timing\n",
        "start_csv = time.time()\n",
        "csv_read_df = spark.read.option(\"header\", \"true\").csv(\"customers.csv\")\n",
        "csv_read_df.count()  # Trigger action\n",
        "end_csv = time.time()\n",
        "\n",
        "# Parquet read timing\n",
        "start_parquet = time.time()\n",
        "parquet_read_df = spark.read.parquet(\"joined_customers_orders.parquet\")\n",
        "parquet_read_df.count()  # Trigger action\n",
        "end_parquet = time.time()\n",
        "\n",
        "print(f\"CSV read time: {end_csv - start_csv:.4f} seconds\")\n",
        "print(f\"Parquet read time: {end_parquet - start_parquet:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJzrP_AxKRM5",
        "outputId": "bbdb6df0-bc1c-491c-f3a6-93422c7ea734"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema of Parquet file:\n",
            "root\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Email: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- SignupDate: date (nullable = true)\n",
            " |-- OrderID: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- OrderDate: date (nullable = true)\n",
            " |-- TotalAmount: double (nullable = true)\n",
            " |-- OrderYear: integer (nullable = true)\n",
            "\n",
            "Data from global temp view:\n",
            "+----------+-----+-----------------+---------+----------+-------+---------+-----------+--------+-------+----------+-----------+---------+\n",
            "|CustomerID| Name|            Email|     City|SignupDate|OrderID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|\n",
            "+----------+-----+-----------------+---------+----------+-------+---------+-----------+--------+-------+----------+-----------+---------+\n",
            "|       101|  Ali|    ali@gmail.com|   Mumbai|2022-05-10|      1|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|\n",
            "|       101|  Ali|    ali@gmail.com|   Mumbai|2022-05-10|      2|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|\n",
            "|       102| Neha|   neha@yahoo.com|    Delhi|2023-01-15|      3|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|\n",
            "|       103| Ravi| ravi@hotmail.com|Bangalore|2021-11-01|      4|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|\n",
            "|       104|Sneha|sneha@outlook.com|Hyderabad|2020-07-22|      5|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|\n",
            "+----------+-----+-----------------+---------+----------+-------+---------+-----------+--------+-------+----------+-----------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "CSV read time: 0.8833 seconds\n",
            "Parquet read time: 0.3830 seconds\n"
          ]
        }
      ]
    }
  ]
}
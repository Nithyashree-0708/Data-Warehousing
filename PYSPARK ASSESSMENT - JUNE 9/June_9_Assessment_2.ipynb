{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "mqHEhU11NAXA",
        "outputId": "b5ab99ee-d2b9-4e89-a1f7-0c7c6b842240"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bb2a3c92-d25f-44da-b32a-832faea01724\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bb2a3c92-d25f-44da-b32a-832faea01724\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving bonuses.json to bonuses.json\n",
            "Saving attendance.csv to attendance.csv\n",
            "Saving employees (1).csv to employees (1).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming\n",
        "!mv 'employees (1).csv' employees.csv"
      ],
      "metadata": {
        "id": "64A-U51iWmnJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import year, month, regexp_replace, concat_ws, lpad, substring, lit, col, when, round, datediff, current_date, count, sum as _sum\n",
        "\n",
        "spark = SparkSession.builder.appName(\"HRAnalytics\").getOrCreate()\n",
        "\n",
        "# Load the datasets\n",
        "employees_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"employees.csv\")\n",
        "attendance_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"attendance.csv\")\n",
        "bonuses_df = spark.read.option(\"multiline\", \"true\").json(\"bonuses.json\")"
      ],
      "metadata": {
        "id": "HV-ywojbWtc_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Ingestion & Exploration**"
      ],
      "metadata": {
        "id": "1vDYhV9KW7wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"HRAnalytics\").getOrCreate()\n",
        "\n",
        "# Read employees.csv\n",
        "employees_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"employees.csv\")\n",
        "\n",
        "# Read attendance.csv\n",
        "attendance_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"attendance.csv\")\n",
        "\n",
        "# Read bonuses.json\n",
        "bonuses_df = spark.read.option(\"multiline\", \"true\").json(\"bonuses.json\")\n",
        "\n",
        "# Show schemas\n",
        "print(\" Employees Schema:\")\n",
        "employees_df.printSchema()\n",
        "\n",
        "print(\"\\n Attendance Schema:\")\n",
        "attendance_df.printSchema()\n",
        "\n",
        "print(\"\\n Bonuses Schema:\")\n",
        "bonuses_df.printSchema()\n",
        "\n",
        "# Show sample records\n",
        "print(\"\\n Sample Employees Data:\")\n",
        "employees_df.show()\n",
        "\n",
        "print(\"\\n Sample Attendance Data:\")\n",
        "attendance_df.show()\n",
        "\n",
        "print(\"\\n Sample Bonuses Data:\")\n",
        "bonuses_df.show()\n",
        "\n",
        "# Count distinct departments\n",
        "print(\"\\n Distinct Departments Count:\")\n",
        "employees_df.select(\"Department\").distinct().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYveGxzaW-P-",
        "outputId": "1da87126-1561-4cc8-fce5-cd85566a8028"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Employees Schema:\n",
            "root\n",
            " |-- EmpID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- JoinDate: date (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            " |-- ManagerID: double (nullable = true)\n",
            "\n",
            "\n",
            " Attendance Schema:\n",
            "root\n",
            " |-- EmpID: integer (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Status: string (nullable = true)\n",
            "\n",
            "\n",
            " Bonuses Schema:\n",
            "root\n",
            " |-- Bonus: long (nullable = true)\n",
            " |-- EmpID: long (nullable = true)\n",
            " |-- Year: long (nullable = true)\n",
            "\n",
            "\n",
            " Sample Employees Data:\n",
            "+-----+------+-----------+----------+------+---------+\n",
            "|EmpID|  Name| Department|  JoinDate|Salary|ManagerID|\n",
            "+-----+------+-----------+----------+------+---------+\n",
            "|    1| Anita|         HR|2021-05-01| 55000|     NULL|\n",
            "|    2|   Raj|Engineering|2020-03-15| 80000|      1.0|\n",
            "|    3|Simran|Engineering|2022-07-10| 75000|      1.0|\n",
            "|    4| Aamir|  Marketing|2019-11-20| 60000|      1.0|\n",
            "|    5| Nisha|         HR|2023-01-05| 50000|      1.0|\n",
            "+-----+------+-----------+----------+------+---------+\n",
            "\n",
            "\n",
            " Sample Attendance Data:\n",
            "+-----+----------+-------+\n",
            "|EmpID|      Date| Status|\n",
            "+-----+----------+-------+\n",
            "|    1|2024-04-01|Present|\n",
            "|    1|2024-04-02|Present|\n",
            "|    2|2024-04-01| Absent|\n",
            "|    2|2024-04-02|Present|\n",
            "|    3|2024-04-01|Present|\n",
            "|    3|2024-04-02|Present|\n",
            "|    4|2024-04-01| Absent|\n",
            "|    4|2024-04-02| Absent|\n",
            "|    5|2024-04-01|Present|\n",
            "|    5|2024-04-02|Present|\n",
            "+-----+----------+-------+\n",
            "\n",
            "\n",
            " Sample Bonuses Data:\n",
            "+-----+-----+----+\n",
            "|Bonus|EmpID|Year|\n",
            "+-----+-----+----+\n",
            "| 5000|    1|2023|\n",
            "| 7000|    2|2023|\n",
            "| 6500|    3|2023|\n",
            "| 6000|    4|2023|\n",
            "| 4000|    5|2023|\n",
            "+-----+-----+----+\n",
            "\n",
            "\n",
            " Distinct Departments Count:\n",
            "+-----------+\n",
            "| Department|\n",
            "+-----------+\n",
            "|Engineering|\n",
            "|         HR|\n",
            "|  Marketing|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: DataFrame Operations**"
      ],
      "metadata": {
        "id": "GPSrkGMxXRBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import datediff, current_date, round, col\n",
        "\n",
        "#  1. Add a column TenureYears using datediff() and round()\n",
        "employees_with_tenure = employees_df.withColumn(\n",
        "    \"TenureYears\",\n",
        "    round(datediff(current_date(), col(\"JoinDate\")) / 365, 2)\n",
        ")\n",
        "\n",
        "print(\" Tenure (in years):\")\n",
        "employees_with_tenure.select(\"EmpID\", \"Name\", \"JoinDate\", \"TenureYears\").show()\n",
        "\n",
        "#  2. Join with bonuses to calculate TotalCompensation = Salary + Bonus\n",
        "emp_bonus_df = employees_with_tenure.join(bonuses_df, on=\"EmpID\", how=\"left\")\n",
        "emp_with_comp = emp_bonus_df.withColumn(\"TotalCompensation\", col(\"Salary\") + col(\"Bonus\"))\n",
        "\n",
        "print(\" Employees with Total Compensation:\")\n",
        "emp_with_comp.select(\"EmpID\", \"Name\", \"Salary\", \"Bonus\", \"TotalCompensation\").show()\n",
        "\n",
        "#  3. Filter employees with more than 2 years in the company\n",
        "print(\" Employees with > 2 years tenure:\")\n",
        "emp_with_comp.filter(col(\"TenureYears\") > 2).select(\"EmpID\", \"Name\", \"TenureYears\").show()\n",
        "\n",
        "#  4. Show employees who report to a manager (ManagerID is not null)\n",
        "print(\" Employees who report to a manager:\")\n",
        "emp_with_comp.filter(col(\"ManagerID\").isNotNull()).select(\"EmpID\", \"Name\", \"ManagerID\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdzsfndMXV04",
        "outputId": "e9ae620e-f54e-4e4e-f069-99fb286a5b69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tenure (in years):\n",
            "+-----+------+----------+-----------+\n",
            "|EmpID|  Name|  JoinDate|TenureYears|\n",
            "+-----+------+----------+-----------+\n",
            "|    1| Anita|2021-05-01|       4.11|\n",
            "|    2|   Raj|2020-03-15|       5.24|\n",
            "|    3|Simran|2022-07-10|       2.92|\n",
            "|    4| Aamir|2019-11-20|       5.56|\n",
            "|    5| Nisha|2023-01-05|       2.43|\n",
            "+-----+------+----------+-----------+\n",
            "\n",
            " Employees with Total Compensation:\n",
            "+-----+------+------+-----+-----------------+\n",
            "|EmpID|  Name|Salary|Bonus|TotalCompensation|\n",
            "+-----+------+------+-----+-----------------+\n",
            "|    1| Anita| 55000| 5000|            60000|\n",
            "|    2|   Raj| 80000| 7000|            87000|\n",
            "|    3|Simran| 75000| 6500|            81500|\n",
            "|    4| Aamir| 60000| 6000|            66000|\n",
            "|    5| Nisha| 50000| 4000|            54000|\n",
            "+-----+------+------+-----+-----------------+\n",
            "\n",
            " Employees with > 2 years tenure:\n",
            "+-----+------+-----------+\n",
            "|EmpID|  Name|TenureYears|\n",
            "+-----+------+-----------+\n",
            "|    1| Anita|       4.11|\n",
            "|    2|   Raj|       5.24|\n",
            "|    3|Simran|       2.92|\n",
            "|    4| Aamir|       5.56|\n",
            "|    5| Nisha|       2.43|\n",
            "+-----+------+-----------+\n",
            "\n",
            " Employees who report to a manager:\n",
            "+-----+------+---------+\n",
            "|EmpID|  Name|ManagerID|\n",
            "+-----+------+---------+\n",
            "|    2|   Raj|      1.0|\n",
            "|    3|Simran|      1.0|\n",
            "|    4| Aamir|      1.0|\n",
            "|    5| Nisha|      1.0|\n",
            "+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Aggregation**"
      ],
      "metadata": {
        "id": "DOMK_r3bXn-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, count\n",
        "\n",
        "# 1. Average salary per department\n",
        "print(\" Average Salary by Department:\")\n",
        "employees_df.groupBy(\"Department\") \\\n",
        "    .agg(avg(\"Salary\").alias(\"AverageSalary\")) \\\n",
        "    .show()\n",
        "\n",
        "#  2. Number of employees under each manager\n",
        "print(\" Employees under each Manager:\")\n",
        "employees_df.groupBy(\"ManagerID\") \\\n",
        "    .agg(count(\"EmpID\").alias(\"NumEmployees\")) \\\n",
        "    .filter(col(\"ManagerID\").isNotNull()) \\\n",
        "    .show()\n",
        "\n",
        "#  3. Count of absences per employee\n",
        "print(\" Absences per Employee:\")\n",
        "attendance_df.filter(col(\"Status\") == \"Absent\") \\\n",
        "    .groupBy(\"EmpID\") \\\n",
        "    .agg(count(\"Status\").alias(\"AbsentDays\")) \\\n",
        "    .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN9QSh_nXvS5",
        "outputId": "a43911e8-b6dc-48f5-a01d-d6999e960f5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Average Salary by Department:\n",
            "+-----------+-------------+\n",
            "| Department|AverageSalary|\n",
            "+-----------+-------------+\n",
            "|Engineering|      77500.0|\n",
            "|         HR|      52500.0|\n",
            "|  Marketing|      60000.0|\n",
            "+-----------+-------------+\n",
            "\n",
            " Employees under each Manager:\n",
            "+---------+------------+\n",
            "|ManagerID|NumEmployees|\n",
            "+---------+------------+\n",
            "|      1.0|           4|\n",
            "+---------+------------+\n",
            "\n",
            " Absences per Employee:\n",
            "+-----+----------+\n",
            "|EmpID|AbsentDays|\n",
            "+-----+----------+\n",
            "|    4|         2|\n",
            "|    2|         1|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4: Joins**"
      ],
      "metadata": {
        "id": "JGxk7TmfX8ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, count, sum as _sum, round\n",
        "\n",
        "#  1. Join employees and attendance → Get attendance % (Present days / Total days)\n",
        "attendance_summary = attendance_df.groupBy(\"EmpID\") \\\n",
        "    .agg(\n",
        "        count(\"Status\").alias(\"TotalDays\"),\n",
        "        _sum(when(col(\"Status\") == \"Present\", 1).otherwise(0)).alias(\"PresentDays\")\n",
        "    ) \\\n",
        "    .withColumn(\"AttendancePercentage\", round((col(\"PresentDays\") / col(\"TotalDays\")) * 100, 2))\n",
        "\n",
        "print(\" Attendance % per Employee:\")\n",
        "attendance_summary.show()\n",
        "\n",
        "#  2. Join with employees to include employee details\n",
        "emp_attendance_df = employees_df.join(attendance_summary, on=\"EmpID\", how=\"left\")\n",
        "\n",
        "#  3. Join with bonuses → Show top 3 employees by TotalCompensation\n",
        "emp_bonus_df = emp_attendance_df.join(bonuses_df, on=\"EmpID\", how=\"left\")\n",
        "emp_bonus_df = emp_bonus_df.withColumn(\"TotalCompensation\", col(\"Salary\") + col(\"Bonus\"))\n",
        "\n",
        "print(\" Top 3 Employees by Total Compensation:\")\n",
        "emp_bonus_df.select(\"EmpID\", \"Name\", \"Salary\", \"Bonus\", \"TotalCompensation\") \\\n",
        "    .orderBy(col(\"TotalCompensation\").desc()) \\\n",
        "    .show(3)\n",
        "\n",
        "#  4. Multi-level join: employees + bonuses + attendance\n",
        "print(\" Full Multi-Join Result:\")\n",
        "emp_bonus_df.select(\"EmpID\", \"Name\", \"Department\", \"TotalDays\", \"PresentDays\", \"AttendancePercentage\", \"TotalCompensation\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5IBmEuKYEOS",
        "outputId": "70016648-a60c-4c5d-cde3-e92dc3f2cdbd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Attendance % per Employee:\n",
            "+-----+---------+-----------+--------------------+\n",
            "|EmpID|TotalDays|PresentDays|AttendancePercentage|\n",
            "+-----+---------+-----------+--------------------+\n",
            "|    1|        2|          2|               100.0|\n",
            "|    3|        2|          2|               100.0|\n",
            "|    5|        2|          2|               100.0|\n",
            "|    4|        2|          0|                 0.0|\n",
            "|    2|        2|          1|                50.0|\n",
            "+-----+---------+-----------+--------------------+\n",
            "\n",
            " Top 3 Employees by Total Compensation:\n",
            "+-----+------+------+-----+-----------------+\n",
            "|EmpID|  Name|Salary|Bonus|TotalCompensation|\n",
            "+-----+------+------+-----+-----------------+\n",
            "|    2|   Raj| 80000| 7000|            87000|\n",
            "|    3|Simran| 75000| 6500|            81500|\n",
            "|    4| Aamir| 60000| 6000|            66000|\n",
            "+-----+------+------+-----+-----------------+\n",
            "only showing top 3 rows\n",
            "\n",
            " Full Multi-Join Result:\n",
            "+-----+------+-----------+---------+-----------+--------------------+-----------------+\n",
            "|EmpID|  Name| Department|TotalDays|PresentDays|AttendancePercentage|TotalCompensation|\n",
            "+-----+------+-----------+---------+-----------+--------------------+-----------------+\n",
            "|    1| Anita|         HR|        2|          2|               100.0|            60000|\n",
            "|    2|   Raj|Engineering|        2|          1|                50.0|            87000|\n",
            "|    3|Simran|Engineering|        2|          2|               100.0|            81500|\n",
            "|    4| Aamir|  Marketing|        2|          0|                 0.0|            66000|\n",
            "|    5| Nisha|         HR|        2|          2|               100.0|            54000|\n",
            "+-----+------+-----------+---------+-----------+--------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5: String & Date Functions**"
      ],
      "metadata": {
        "id": "XzL6jrw8YXgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, month, regexp_replace, concat_ws, lpad, substring\n",
        "\n",
        "#  1. Extract year and month from JoinDate\n",
        "print(\" Year and Month from JoinDate:\")\n",
        "employees_df.withColumn(\"JoinYear\", year(\"JoinDate\")) \\\n",
        "    .withColumn(\"JoinMonth\", month(\"JoinDate\")) \\\n",
        "    .select(\"EmpID\", \"Name\", \"JoinDate\", \"JoinYear\", \"JoinMonth\") \\\n",
        "    .show()\n",
        "\n",
        "#  2. Mask employee names using regex (e.g., A***)\n",
        "print(\" Masked Employee Names:\")\n",
        "masked_names_df = employees_df.withColumn(\"MaskedName\", regexp_replace(\"Name\", r\"(?<=^.).\", \"*\"))\n",
        "masked_names_df.select(\"EmpID\", \"Name\", \"MaskedName\").show()\n",
        "\n",
        "#  3. Use substring() to create EmpCode like \"EMP001\"\n",
        "print(\" Employee Codes:\")\n",
        "emp_code_df = employees_df.withColumn(\"EmpCode\", concat_ws(\"\", lit(\"EMP\"), lpad(col(\"EmpID\").cast(\"string\"), 3, \"0\")))\n",
        "emp_code_df.select(\"EmpID\", \"Name\", \"EmpCode\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrnhkUDAYdHi",
        "outputId": "5e6ce3ff-e96c-4c3c-c7eb-e0202e450dff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Year and Month from JoinDate:\n",
            "+-----+------+----------+--------+---------+\n",
            "|EmpID|  Name|  JoinDate|JoinYear|JoinMonth|\n",
            "+-----+------+----------+--------+---------+\n",
            "|    1| Anita|2021-05-01|    2021|        5|\n",
            "|    2|   Raj|2020-03-15|    2020|        3|\n",
            "|    3|Simran|2022-07-10|    2022|        7|\n",
            "|    4| Aamir|2019-11-20|    2019|       11|\n",
            "|    5| Nisha|2023-01-05|    2023|        1|\n",
            "+-----+------+----------+--------+---------+\n",
            "\n",
            " Masked Employee Names:\n",
            "+-----+------+----------+\n",
            "|EmpID|  Name|MaskedName|\n",
            "+-----+------+----------+\n",
            "|    1| Anita|     A*ita|\n",
            "|    2|   Raj|       R*j|\n",
            "|    3|Simran|    S*mran|\n",
            "|    4| Aamir|     A*mir|\n",
            "|    5| Nisha|     N*sha|\n",
            "+-----+------+----------+\n",
            "\n",
            " Employee Codes:\n",
            "+-----+------+-------+\n",
            "|EmpID|  Name|EmpCode|\n",
            "+-----+------+-------+\n",
            "|    1| Anita| EMP001|\n",
            "|    2|   Raj| EMP002|\n",
            "|    3|Simran| EMP003|\n",
            "|    4| Aamir| EMP004|\n",
            "|    5| Nisha| EMP005|\n",
            "+-----+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6: Conditional & Null Handling**"
      ],
      "metadata": {
        "id": "zO312_zyZ8zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# 1. Use when/otherwise to label performance based on Bonus:\n",
        "performance_df = emp_bonus_df.withColumn(\n",
        "    \"Performance\",\n",
        "    when(col(\"Bonus\") > 6000, \"High\")\n",
        "    .when((col(\"Bonus\") >= 4000) & (col(\"Bonus\") <= 6000), \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "print(\" Employee Performance Labels:\")\n",
        "performance_df.select(\"EmpID\", \"Name\", \"Bonus\", \"Performance\").show()\n",
        "\n",
        "# 2. Handle missing ManagerID by filling nulls with \"No Manager\"\n",
        "# ManagerID is numeric, so convert nulls to a string label in a new column for display\n",
        "\n",
        "performance_df = performance_df.withColumn(\n",
        "    \"ManagerID_Display\",\n",
        "    when(col(\"ManagerID\").isNull(), \"No Manager\").otherwise(col(\"ManagerID\").cast(\"string\"))\n",
        ")\n",
        "\n",
        "print(\" Manager Info with Null Handling:\")\n",
        "performance_df.select(\"EmpID\", \"Name\", \"ManagerID\", \"ManagerID_Display\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SCdJahQaBoN",
        "outputId": "2dca0d91-f077-4558-8658-a8d14bac782a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Employee Performance Labels:\n",
            "+-----+------+-----+-----------+\n",
            "|EmpID|  Name|Bonus|Performance|\n",
            "+-----+------+-----+-----------+\n",
            "|    1| Anita| 5000|     Medium|\n",
            "|    2|   Raj| 7000|       High|\n",
            "|    3|Simran| 6500|       High|\n",
            "|    4| Aamir| 6000|     Medium|\n",
            "|    5| Nisha| 4000|     Medium|\n",
            "+-----+------+-----+-----------+\n",
            "\n",
            " Manager Info with Null Handling:\n",
            "+-----+------+---------+-----------------+\n",
            "|EmpID|  Name|ManagerID|ManagerID_Display|\n",
            "+-----+------+---------+-----------------+\n",
            "|    1| Anita|     NULL|       No Manager|\n",
            "|    2|   Raj|      1.0|              1.0|\n",
            "|    3|Simran|      1.0|              1.0|\n",
            "|    4| Aamir|      1.0|              1.0|\n",
            "|    5| Nisha|      1.0|              1.0|\n",
            "+-----+------+---------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 7: Spark SQL**"
      ],
      "metadata": {
        "id": "X_5KakiNaO7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create and use database hr\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS hr\")\n",
        "spark.sql(\"USE hr\")\n",
        "\n",
        "# 2. Save DataFrames as tables in the hr database\n",
        "employees_df.write.mode(\"overwrite\").saveAsTable(\"employees\")\n",
        "attendance_df.write.mode(\"overwrite\").saveAsTable(\"attendance\")\n",
        "bonuses_df.write.mode(\"overwrite\").saveAsTable(\"bonuses\")\n",
        "\n",
        "# 3. Write and run SQL queries:\n",
        "\n",
        "# a. Top paid employee in each department\n",
        "print(\" Top Paid Employee in Each Department:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT Department, Name, Salary\n",
        "    FROM (\n",
        "        SELECT *, ROW_NUMBER() OVER (PARTITION BY Department ORDER BY Salary DESC) AS rn\n",
        "        FROM employees\n",
        "    )\n",
        "    WHERE rn = 1\n",
        "\"\"\").show()\n",
        "\n",
        "# b. Attendance rate by department\n",
        "print(\" Attendance Rate by Department:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT e.Department,\n",
        "           ROUND(SUM(CASE WHEN a.Status = 'Present' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) AS AttendanceRate\n",
        "    FROM employees e\n",
        "    JOIN attendance a ON e.EmpID = a.EmpID\n",
        "    GROUP BY e.Department\n",
        "\"\"\").show()\n",
        "\n",
        "# c. Employees joined after 2021 with salary > 70,000\n",
        "print(\" Employees Joined After 2021 with Salary > 70,000:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT Name, Department, JoinDate, Salary\n",
        "    FROM employees\n",
        "    WHERE JoinDate > '2021-12-31' AND Salary > 70000\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzjRX7l2aRem",
        "outputId": "6064580e-cc0d-4292-b951-e7607c48e773"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Top Paid Employee in Each Department:\n",
            "+-----------+-----+------+\n",
            "| Department| Name|Salary|\n",
            "+-----------+-----+------+\n",
            "|Engineering|  Raj| 80000|\n",
            "|         HR|Anita| 55000|\n",
            "|  Marketing|Aamir| 60000|\n",
            "+-----------+-----+------+\n",
            "\n",
            " Attendance Rate by Department:\n",
            "+-----------+--------------+\n",
            "| Department|AttendanceRate|\n",
            "+-----------+--------------+\n",
            "|Engineering|         75.00|\n",
            "|         HR|        100.00|\n",
            "|  Marketing|          0.00|\n",
            "+-----------+--------------+\n",
            "\n",
            " Employees Joined After 2021 with Salary > 70,000:\n",
            "+------+-----------+----------+------+\n",
            "|  Name| Department|  JoinDate|Salary|\n",
            "+------+-----------+----------+------+\n",
            "|Simran|Engineering|2022-07-10| 75000|\n",
            "+------+-----------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 8: Advanced (Optional)**\n"
      ],
      "metadata": {
        "id": "hW4G-BkQcH_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Define UDF\n",
        "def classify_department(dept):\n",
        "    return \"Tech\" if dept in [\"Engineering\", \"IT\"] else \"Non-Tech\"\n",
        "\n",
        "# Register UDF\n",
        "classify_udf = udf(classify_department, StringType())\n",
        "\n",
        "# Apply to employees_df\n",
        "classified_df = employees_df.withColumn(\"DeptType\", classify_udf(col(\"Department\")))\n",
        "\n",
        "print(\" Department Classification:\")\n",
        "classified_df.select(\"EmpID\", \"Name\", \"Department\", \"DeptType\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI-dpu3ycKUm",
        "outputId": "176d6e58-9b80-43cd-f5ad-c632f8a28b21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Department Classification:\n",
            "+-----+------+-----------+--------+\n",
            "|EmpID|  Name| Department|DeptType|\n",
            "+-----+------+-----------+--------+\n",
            "|    1| Anita|         HR|Non-Tech|\n",
            "|    2|   Raj|Engineering|    Tech|\n",
            "|    3|Simran|Engineering|    Tech|\n",
            "|    4| Aamir|  Marketing|Non-Tech|\n",
            "|    5| Nisha|         HR|Non-Tech|\n",
            "+-----+------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, when, col, sum as _sum\n",
        "\n",
        "# Join to get attendance count\n",
        "summary_df = attendance_df.groupBy(\"EmpID\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"TotalDays\"),\n",
        "        _sum(when(col(\"Status\") == \"Present\", 1).otherwise(0)).alias(\"PresentDays\")\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "7gSZYnD1cTs7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, when, col, sum as _sum\n",
        "\n",
        "# Step 1: Recalculate attendance summary\n",
        "summary_df = attendance_df.groupBy(\"EmpID\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"TotalDays\"),\n",
        "        _sum(when(col(\"Status\") == \"Present\", 1).otherwise(0)).alias(\"PresentDays\")\n",
        "    )\n",
        "\n",
        "# Step 2: Join with employees DataFrame\n",
        "emp_attendance_summary = employees_df.join(summary_df, on=\"EmpID\", how=\"left\")\n",
        "\n",
        "# Step 3: Save as Parquet partitioned by Department\n",
        "emp_attendance_summary.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"Department\") \\\n",
        "    .parquet(\"/content/emp_attendance_summary_parquet\")\n",
        "\n",
        "print(\" emp_attendance_summary saved as Parquet partitioned by Department.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sSBHr62cwE5",
        "outputId": "1068f6a6-c281-4dee-9032-110045e7dbb2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " emp_attendance_summary saved as Parquet partitioned by Department.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet = spark.read.parquet(\"/content/emp_attendance_summary_parquet\")\n",
        "df_parquet.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_54P3QDwdZFz",
        "outputId": "32583791-6fc4-4a42-aaaf-c6cecfdd2d55"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+---------+---------+-----------+-----------+\n",
            "|EmpID|  Name|  JoinDate|Salary|ManagerID|TotalDays|PresentDays| Department|\n",
            "+-----+------+----------+------+---------+---------+-----------+-----------+\n",
            "|    2|   Raj|2020-03-15| 80000|      1.0|        2|          1|Engineering|\n",
            "|    3|Simran|2022-07-10| 75000|      1.0|        2|          2|Engineering|\n",
            "|    1| Anita|2021-05-01| 55000|     NULL|        2|          2|         HR|\n",
            "|    5| Nisha|2023-01-05| 50000|      1.0|        2|          2|         HR|\n",
            "|    4| Aamir|2019-11-20| 60000|      1.0|        2|          0|  Marketing|\n",
            "+-----+------+----------+------+---------+---------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
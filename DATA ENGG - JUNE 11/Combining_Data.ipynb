{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JMtuZLLiYhqT"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, count, max, min, when, rank, sum as _sum, current_date, datediff\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lit, expr\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedEmployeeData\").getOrCreate()\n",
        "\n",
        "# Project dataset\n",
        "project_data = [\n",
        "    (\"Ananya\", \"HR Portal\", 120),\n",
        "    (\"Rahul\", \"Data Platform\", 200),\n",
        "    (\"Priya\", \"Data Platform\", 180),\n",
        "    (\"Zoya\", \"Campaign Tracker\", 100),\n",
        "    (\"Karan\", \"HR Portal\", 130),\n",
        "    (\"Naveen\", \"ML Pipeline\", 220),\n",
        "    (\"Fatima\", \"Campaign Tracker\", 90)\n",
        "]\n",
        "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
        "df_proj = spark.createDataFrame(project_data, columns_proj)\n",
        "\n",
        "# Recreate employee_data\n",
        "data = [\n",
        "    (\"Ananya\", \"HR\", 52000),\n",
        "    (\"Rahul\", \"Engineering\", 65000),\n",
        "    (\"Priya\", \"Engineering\", 60000),\n",
        "    (\"Zoya\", \"Marketing\", 48000),\n",
        "    (\"Karan\", \"HR\", 53000),\n",
        "    (\"Naveen\", \"Engineering\", 70000),\n",
        "    (\"Fatima\", \"Marketing\", 45000)\n",
        "]\n",
        "columns = [\"Name\", \"Department\", \"Salary\"]\n",
        "df_emp = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Recreate performance_data\n",
        "performance = [\n",
        "    (\"Ananya\", 2023, 4.5),\n",
        "    (\"Rahul\", 2023, 4.9),\n",
        "    (\"Priya\", 2023, 4.3),\n",
        "    (\"Zoya\", 2023, 3.8),\n",
        "    (\"Karan\", 2023, 4.1),\n",
        "    (\"Naveen\", 2023, 4.7),\n",
        "    (\"Fatima\", 2023, 3.9)\n",
        "]\n",
        "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
        "df_perf = spark.createDataFrame(performance, columns_perf)\n",
        "\n",
        "# Recreate project_data\n",
        "project_data = [\n",
        "    (\"Ananya\", \"HR Portal\", 120),\n",
        "    (\"Rahul\", \"Data Platform\", 200),\n",
        "    (\"Priya\", \"Data Platform\", 180),\n",
        "    (\"Zoya\", \"Campaign Tracker\", 100),\n",
        "    (\"Karan\", \"HR Portal\", 130),\n",
        "    (\"Naveen\", \"ML Pipeline\", 220),\n",
        "    (\"Fatima\", \"Campaign Tracker\", 90)\n",
        "]\n",
        "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
        "df_proj = spark.createDataFrame(project_data, columns_proj)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Join all three datasets on Name\n",
        "df_all = df_emp.join(df_perf, on=\"Name\", how=\"inner\") \\\n",
        "               .join(df_proj, on=\"Name\", how=\"inner\")\n",
        "df_all.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eDkvwqHa5zP",
        "outputId": "e76650d4-cfca-44e4-8b33-3b3e9c59c34f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+------+----------------+-----------+\n",
            "|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n",
            "+------+-----------+------+----+------+----------------+-----------+\n",
            "|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n",
            "| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n",
            "|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n",
            "|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n",
            "|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n",
            "| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n",
            "+------+-----------+------+----+------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Total hours worked per department\n",
        "df_all.groupBy(\"Department\").sum(\"HoursWorked\").withColumnRenamed(\"sum(HoursWorked)\", \"TotalHours\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg5ZACKycf8E",
        "outputId": "93ba4ed4-9f7e-487e-a39e-782fc152709e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "| Department|TotalHours|\n",
            "+-----------+----------+\n",
            "|Engineering|       600|\n",
            "|         HR|       250|\n",
            "|  Marketing|       190|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Average rating per project\n",
        "df_all.groupBy(\"Project\").avg(\"Rating\").withColumnRenamed(\"avg(Rating)\", \"AvgRating\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKu_t1HMcwhl",
        "outputId": "58912769-4a93-4235-ace8-09cc7ba224f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------------------+\n",
            "|         Project|         AvgRating|\n",
            "+----------------+------------------+\n",
            "|   Data Platform|               4.6|\n",
            "|       HR Portal|               4.3|\n",
            "|     ML Pipeline|               4.7|\n",
            "|Campaign Tracker|3.8499999999999996|\n",
            "+----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  4. Add a row with a None rating\n",
        "from pyspark.sql import Row\n",
        "\n",
        "new_row = Row(\"Meena\", 2023, None)\n",
        "df_perf_null = df_perf.union(spark.createDataFrame([new_row], df_perf.schema))\n",
        "df_perf_null.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeSm1alBc5r7",
        "outputId": "7f9651a5-1e16-4308-baf4-cb47b6397796"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+------+\n",
            "|  Name|Year|Rating|\n",
            "+------+----+------+\n",
            "|Ananya|2023|   4.5|\n",
            "| Rahul|2023|   4.9|\n",
            "| Priya|2023|   4.3|\n",
            "|  Zoya|2023|   3.8|\n",
            "| Karan|2023|   4.1|\n",
            "|Naveen|2023|   4.7|\n",
            "|Fatima|2023|   3.9|\n",
            "| Meena|2023|  NULL|\n",
            "+------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # 5. Filter rows with null values\n",
        " df_perf_null.filter(col(\"Rating\").isNull()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLMkV_jYdCP1",
        "outputId": "dedfce3b-546e-4445-ed6f-be47c08043f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+------+\n",
            "| Name|Year|Rating|\n",
            "+-----+----+------+\n",
            "|Meena|2023|  NULL|\n",
            "+-----+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Replace null ratings with department average\n",
        "df_perf_null_joined = df_perf_null.join(df_emp, on=\"Name\", how=\"left\")"
      ],
      "metadata": {
        "id": "Fu8LiGavdLwu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg, when\n",
        "\n",
        "windowSpec = Window.partitionBy(\"Department\")\n",
        "df_filled = df_perf_null_joined.withColumn(\n",
        "    \"FilledRating\",\n",
        "    when(col(\"Rating\").isNull(), avg(\"Rating\").over(windowSpec)).otherwise(col(\"Rating\"))\n",
        ")\n",
        "df_filled.select(\"Name\", \"Department\", \"Rating\", \"FilledRating\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krvtmNsidc9g",
        "outputId": "46cbeaee-673a-4d81-dc20-7bad89f512b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+------------+\n",
            "|  Name| Department|Rating|FilledRating|\n",
            "+------+-----------+------+------------+\n",
            "| Meena|       NULL|  NULL|        NULL|\n",
            "| Priya|Engineering|   4.3|         4.3|\n",
            "| Rahul|Engineering|   4.9|         4.9|\n",
            "|Naveen|Engineering|   4.7|         4.7|\n",
            "|Ananya|         HR|   4.5|         4.5|\n",
            "| Karan|         HR|   4.1|         4.1|\n",
            "|Fatima|  Marketing|   3.9|         3.9|\n",
            "|  Zoya|  Marketing|   3.8|         3.8|\n",
            "+------+-----------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Create PerformanceCategory column\n",
        "df_all = df_all.withColumn(\"PerformanceCategory\",\n",
        "    when(col(\"Rating\") >= 4.7, \"Excellent\")\n",
        "    .when((col(\"Rating\") >= 4.0) & (col(\"Rating\") < 4.7), \"Good\")\n",
        "    .otherwise(\"Average\")\n",
        ")\n",
        "df_all.select(\"Name\", \"Rating\", \"PerformanceCategory\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXKjZ5T4djtw",
        "outputId": "08bbd62a-30c7-4a02-c2e0-5f1344971327"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------------------+\n",
            "|  Name|Rating|PerformanceCategory|\n",
            "+------+------+-------------------+\n",
            "|Ananya|   4.5|               Good|\n",
            "| Priya|   4.3|               Good|\n",
            "| Rahul|   4.9|          Excellent|\n",
            "|Naveen|   4.7|          Excellent|\n",
            "|Fatima|   3.9|            Average|\n",
            "|  Zoya|   3.8|            Average|\n",
            "| Karan|   4.1|               Good|\n",
            "+------+------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  8. Create UDF to assign bonus\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def bonus_calc(hours):\n",
        "    return 10000 if hours > 200 else 5000\n",
        "\n",
        "bonus_udf = udf(bonus_calc, IntegerType())\n",
        "\n",
        "df_all = df_all.withColumn(\"Bonus\", bonus_udf(col(\"HoursWorked\")))\n",
        "df_all.select(\"Name\", \"HoursWorked\", \"Bonus\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C448GlYNdtpz",
        "outputId": "1b7843c6-1ff6-4992-afcb-95345a6c5727"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+-----+\n",
            "|  Name|HoursWorked|Bonus|\n",
            "+------+-----------+-----+\n",
            "|Ananya|        120| 5000|\n",
            "| Priya|        180| 5000|\n",
            "| Rahul|        200| 5000|\n",
            "|Naveen|        220|10000|\n",
            "|Fatima|         90| 5000|\n",
            "|  Zoya|        100| 5000|\n",
            "| Karan|        130| 5000|\n",
            "+------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  9. Add JoinDate and MonthsWorked\n",
        "from pyspark.sql.functions import to_date, months_between, current_date\n",
        "\n",
        "df_all = df_all.withColumn(\"JoinDate\", to_date(lit(\"2021-06-01\")))\n",
        "df_all = df_all.withColumn(\"MonthsWorked\", months_between(current_date(), col(\"JoinDate\")).cast(\"int\"))\n",
        "df_all.select(\"Name\", \"JoinDate\", \"MonthsWorked\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7plPgptd22Z",
        "outputId": "e6e9b5bb-dce3-498b-b47f-773d8d07a2cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+------------+\n",
            "|  Name|  JoinDate|MonthsWorked|\n",
            "+------+----------+------------+\n",
            "|Ananya|2021-06-01|          48|\n",
            "| Priya|2021-06-01|          48|\n",
            "| Rahul|2021-06-01|          48|\n",
            "|Naveen|2021-06-01|          48|\n",
            "|Fatima|2021-06-01|          48|\n",
            "|  Zoya|2021-06-01|          48|\n",
            "| Karan|2021-06-01|          48|\n",
            "+------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Count employees who joined before 2022\n",
        "df_all.filter(col(\"JoinDate\") < to_date(lit(\"2022-01-01\"))).select(\"Name\").distinct().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ3qlWJwd_F1",
        "outputId": "bc991d6a-f994-4992-a162-ac045e44f96f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Union with extra team members\n",
        "extra_employees = [\n",
        "    (\"Meena\", \"HR\", 48000),\n",
        "    (\"Raj\", \"Marketing\", 51000)\n",
        "]\n",
        "df_extra = spark.createDataFrame(extra_employees, [\"Name\", \"Department\", \"Salary\"])\n",
        "df_combined = df_emp.union(df_extra)\n",
        "df_combined.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDh5A2TWeLov",
        "outputId": "1b2b4853-0b99-49b0-e721-eb2476102640"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+\n",
            "|  Name| Department|Salary|\n",
            "+------+-----------+------+\n",
            "|Ananya|         HR| 52000|\n",
            "| Rahul|Engineering| 65000|\n",
            "| Priya|Engineering| 60000|\n",
            "|  Zoya|  Marketing| 48000|\n",
            "| Karan|         HR| 53000|\n",
            "|Naveen|Engineering| 70000|\n",
            "|Fatima|  Marketing| 45000|\n",
            "| Meena|         HR| 48000|\n",
            "|   Raj|  Marketing| 51000|\n",
            "+------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Save final merged dataset partitioned by department\n",
        "df_all.write.mode(\"overwrite\").partitionBy(\"Department\").parquet(\"/content/final_employee_dataset_partitioned\")"
      ],
      "metadata": {
        "id": "Y3S5Q-3MeTYy"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
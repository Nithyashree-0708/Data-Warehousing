{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bebb2f64-f40b-4302-ae29-fdfc1ad0b7ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1407818079788496#setting/sparkui/0611-043343-dvbo736r/driver-790494521946126491\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x710f577d9cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Set-1\").getOrCreate()\n",
    "\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8462d14e-5dff-4745-ad38-488dcab333dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------------------------------------------------+------+------+\n|OrderID|Customer|Items                                                         |Region|Amount|\n+-------+--------+--------------------------------------------------------------+------+------+\n|101    |Ali     |[{Product -> Laptop, Qty -> 1}, {Product -> Mouse, Qty -> 2}] |Asia  |1200.0|\n|102    |Zara    |[{Product -> Tablet, Qty -> 1}]                               |Europe|650.0 |\n|103    |Mohan   |[{Product -> Phone, Qty -> 2}, {Product -> Charger, Qty -> 1}]|Asia  |890.0 |\n|104    |Sara    |[{Product -> Desk, Qty -> 1}]                                 |US    |450.0 |\n+-------+--------+--------------------------------------------------------------+------+------+\n\n+-------+--------+--------------------+------+------+\n|OrderID|Customer|               Items|Region|Amount|\n+-------+--------+--------------------+------+------+\n|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|\n|    102|    Zara|[{Product -> Tabl...|Europe| 650.0|\n|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|\n|    104|    Sara|[{Product -> Desk...|    US| 450.0|\n+-------+--------+--------------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "data = [\n",
    "Row(OrderID=101, Customer=\"Ali\", Items=[{\"Product\":\"Laptop\", \"Qty\":1},\n",
    "{\"Product\":\"Mouse\", \"Qty\":2}], Region=\"Asia\", Amount=1200.0),\n",
    "Row(OrderID=102, Customer=\"Zara\", Items=[{\"Product\":\"Tablet\", \"Qty\":1}],\n",
    "Region=\"Europe\", Amount=650.0),\n",
    "Row(OrderID=103, Customer=\"Mohan\", Items=[{\"Product\":\"Phone\", \"Qty\":2},\n",
    "{\"Product\":\"Charger\", \"Qty\":1}], Region=\"Asia\", Amount=890.0),\n",
    "Row(OrderID=104, Customer=\"Sara\", Items=[{\"Product\":\"Desk\", \"Qty\":1}],\n",
    "Region=\"US\", Amount=450.0)\n",
    "]\n",
    "df_sales = spark.createDataFrame(data)\n",
    "df_sales.show(truncate=False)\n",
    "\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3fd8fda-3052-4722-990d-78f7ee375781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**PySpark Exercises â€“ Set 4 (SQL, JSON, AdvancedFunctions)\n",
    " Working with JSON & Nested Fields**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4afb5de6-bfcb-4c59-9721-35f7e8948397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|Product|Quantity|\n+-------+--------+\n|Laptop |1       |\n|Mouse  |2       |\n|Tablet |1       |\n|Phone  |2       |\n|Charger|1       |\n|Desk   |1       |\n+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Flatten the Items array using explode() to create one row per product.\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_flat = df_sales.withColumn(\"Item\",explode(\"Items\"))\n",
    "df_flat = df_flat.select(df_flat[\"Item.Product\"].alias(\"Product\"), df_flat[\"Item.Qty\"].alias(\"Quantity\"))\n",
    "df_flat.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dfab6fd-76cf-417c-8334-4f2e4728897d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n|Product|SummedQuantity|\n+-------+--------------+\n| Laptop|           1.0|\n|  Mouse|           2.0|\n| Tablet|           1.0|\n|  Phone|           2.0|\n|Charger|           1.0|\n|   Desk|           1.0|\n+-------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Count total quantity sold per product.\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_count = df_flat.groupBy(\"Product\").agg(sum(\"Quantity\").alias(\"SummedQuantity\"))\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056f73b4-eaa5-45dd-821f-f12a9a1c64bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n|Region|DistinctOrders|\n+------+--------------+\n|Europe|             1|\n|    US|             1|\n|  Asia|             2|\n+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Count number of orders per region.\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df_count = df_sales.groupBy(\"Region\").agg(countDistinct(\"OrderID\").alias(\"DistinctOrders\"))\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa4e5004-5fe8-45cc-b2bf-6a5eaab9cbbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Using when and otherwise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0746f638-b3d8-46f3-8fae-5c55fd706f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+------+------+--------------+\n|OrderID|Customer|               Items|Region|Amount|HighValueOrder|\n+-------+--------+--------------------+------+------+--------------+\n|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|           Yes|\n|    102|    Zara|[{Product -> Tabl...|Europe| 650.0|            No|\n|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|            No|\n|    104|    Sara|[{Product -> Desk...|    US| 450.0|            No|\n+-------+--------+--------------------+------+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Create a new column HighValueOrder :\n",
    "# \"Yes\" if Amount > 1000\n",
    "# \"No\" otherwise\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_sales = df_sales.withColumn(\"HighValueOrder\",when(df_sales[\"Amount\"] > 1000, \"Yes\").otherwise(\"No\"))\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0097be26-e288-467d-8e4b-2cd2a09a474b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+------+------+--------------+------------+\n|OrderID|Customer|               Items|Region|Amount|HighValueOrder|ShippingZone|\n+-------+--------+--------------------+------+------+--------------+------------+\n|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|           Yes|      Zone A|\n|    102|    Zara|[{Product -> Tabl...|Europe| 650.0|            No|      Zone B|\n|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|            No|      Zone A|\n|    104|    Sara|[{Product -> Desk...|    US| 450.0|            No|      Zone C|\n+-------+--------+--------------------+------+------+--------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Add a column ShippingZone :\n",
    "# Asia â†’ \"Zone A\", Europe â†’ \"Zone B\", US â†’ \"Zone C\"\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_sales = df_sales.withColumn(\"ShippingZone\",when(df_sales[\"Region\"] == \"Asia\", \"Zone A\").when(df_sales[\"Region\"] == \"Europe\", \"Zone B\").when(df_sales[\"Region\"] == \"US\", \"Zone C\").otherwise(\"Zone D\"))\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a80c60e-aaea-4e8f-a7ac-1b024e34d015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Temporary & Permanent Views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80db0575-d804-4e8d-a69e-1e611245e8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Register df_sales as a temporary view named sales_view .\n",
    "\n",
    "df_sales.createOrReplaceTempView(\"sales_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a251e3a6-5ac6-4865-830c-cfa5d7c82a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|Region|countorders|avgamt|\n+------+-----------+------+\n|  Asia|          2|1045.0|\n|Europe|          1| 650.0|\n|    US|          1| 450.0|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 7. Write a SQL query to:\n",
    "# Count orders by Region\n",
    "# Find average amount per region\n",
    "\n",
    "spark.sql(\"\"\"SELECT Region, count(OrderID) AS countorders, avg(Amount) as avgamt from sales_view GROUP BY Region\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446ad1fa-01da-4105-8a0d-747300f3b09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. Create a permanent view using saveAsTable() .\n",
    "df_sales.write.saveAsTable(\"sales_permanent_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29017dfb-fe2c-487c-8953-976a5033351d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Queries via Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f5fa0be-dd1f-4458-acd6-b85d6760ed85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|Region|OrderCount|\n+------+----------+\n|  Asia|         2|\n|Europe|         1|\n|    US|         1|\n+------+----------+\n\n+-------+--------+--------------------+------+------+--------------+------------+\n|OrderID|Customer|               Items|Region|Amount|HighValueOrder|ShippingZone|\n+-------+--------+--------------------+------+------+--------------+------------+\n|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|           Yes|      Zone A|\n|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|            No|      Zone A|\n+-------+--------+--------------------+------+------+--------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"SELECT Region, COUNT(*) as OrderCount FROM sales_view GROUP BYRegion\").show()\n",
    "# 9. Use SQL to filter all orders with more than 1 item.\n",
    "\n",
    "spark.sql(\"SELECT Region, COUNT(*) as OrderCount FROM sales_view GROUP BY Region\").show()\n",
    "\n",
    "spark.sql(\"Select * from sales_view where size(Items) > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5b4d67-18c1-4035-ab86-3aa97aae4ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|Customer|\n+--------+\n|     Ali|\n|   Mohan|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 10. Use SQL to extract customer names where Amount > 800.\n",
    "\n",
    "spark.sql(\"select Customer from sales_view where Amount > 800\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1be922f5-9653-4489-81e1-25baad05d396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Saving as Parquet and Reading Again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15ac2d3b-812b-42fc-8a4c-4b642e66ccdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11. Save the exploded product-level DataFrame as a partitioned Parquet file by Region .\n",
    "\n",
    "# Save to DBFS path, partitioned by Region\n",
    "df_flat.write.mode(\"overwrite\").parquet(\"/tmp/product_sales_by_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf88db3-4ff5-4394-bc60-3123ba4b0c42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|Product|TotalQty|\n+-------+--------+\n|  Phone|     2.0|\n|Charger|     1.0|\n| Laptop|     1.0|\n|  Mouse|     2.0|\n| Tablet|     1.0|\n|   Desk|     1.0|\n+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 12. Read the parquet back and perform a group-by on Product .\n",
    "\n",
    "df_parquet = spark.read.parquet(\"/tmp/product_sales_by_region\")\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_parquet.groupBy(\"Product\").agg(sum(\"Quantity\").alias(\"TotalQty\")).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June 12 - Set 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
